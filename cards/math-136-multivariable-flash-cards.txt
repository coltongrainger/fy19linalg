<latex> Definition (13.1.1): The Distance Formula in Three Dimensions </latex>	<latex> $d(P_{1}, P_{2}) = \sqrt{x_{2} - x_{1})^{2} + (y_{2} - y_{1})^{2} + (z_{2} - z_{1})^{2}}$ </latex>
<latex> Definition (13.1.2): A Sphere of Radius $r$ </latex>	<latex> $(x - a)^{2} + (y-b)^{2} + (z-c)^{2} = r^{2}$ with the center at $P_{0} = (a, b, c)$ </latex>
<latex> Definition (13.1.3): Equation of a Sphere with Center $P_{0} = (0, 0, 0)$ </latex>	<latex> $x^{2} + y^{2} + z^{2} = r^{2}$ </latex>
<latex> Theorem (13.1.4): Line Between Two Points </latex>	<latex> The line $l$ that passes through the points $A(a_{2}, a_{2}, a_{3})$ and $B(b_{2}, b_{2}, b_{3})$ consistsof all points $P(x, y, z)$ with$x = a_{1} + t(b_{1} - a_{1})$, $y = a_{2} + t(b_{2} - a_{2})$, $x = a_{3} + t(b_{3} - a_{3})$ and$t$ ranging over the set of real numbers. </latex>
<latex> Theorem (13.1.5): Line Between Two Points on $t \in (0, 1)$ </latex>	<latex> The line segment $\overline{AB}$, as $t$ ranges from $0$ to $1$, the points $P(x, y, z)$ with$\\$ $x = a_{1} + t(b_{1} - a_{1})$, $y = a_{2} + t(b_{2} - a_{2})$, $x = a_{3} + t(b_{3} - a_{3})$$\\$ trace out the line segment $\overline{AB}$ </latex>
<latex> Theorem (13.1.6): The Midpoint Formula </latex>	<latex> The midpoint of $\overline{AB}$ has coordinates$\\$ $x = \frac{1}{2}(a_{1} + b_{1})$,  $y = \frac{1}{2}(a_{2} + b_{2})$, $z = \frac{1}{2}(a_{3} + b_{3})$ </latex>
<latex> Definition (15.4.1): Partial Derivatives (Two Variables) </latex>	<latex> Let $f$ be a function of two variables x, y. The partial derivatives of $f$ with respect to $x$ and with respect to $y$ are the functions $$f_{x}(x, y) = \lim_{h\to0}\frac{f(x+h, y) - f(x,y)}{h}$$ $$f_{y}(x, y) = \lim_{h\to0}\frac{f(x, y+h) - f(x,y)}{h}$$ provided that these limits exist. </latex>
<latex> Definition (15.4.2): Partial Derivatives (Three Variables) </latex>	<latex> Let $f$ be a function of two variables x, y, z. The partial derivatives of $f$ with respect to $x$, $y$, and $y$ are the functions $$f_{x}(x, y, z) = \lim_{h\to0}\frac{f(x+h, y, z) - f(x,y, z)}{h}$$ $$f_{y}(x, y, z) = \lim_{h\to0}\frac{f(x, y+h, z) - f(x,y, z)}{h}$$ $$f_{z}(x, y, z) = \lim_{h\to0}\frac{f(x, y, z+h) - f(x,y, z)}{h}$$  provided that these limits exist. </latex>
<latex> Definition (15.5.1): Neighborhood of a Point </latex>	<latex> A neighborhood of a point $x_{0}$ is a set of the form${\overrightarrow{x}: ||\overrightarrow{x} - \overrightarrow{x_{0}}|| < \delta}$where $\delta$ is some number greater than zero. </latex>
<latex> Definition (15.5.2): The Interior of a Set </latex>	<latex> A point $\overline{x_{0}}$ is said to be an interior point of the set $S$ if the set $S$ contains someneighborhood of $\overline{x_{0}}$. The set of all interior points of $S$ is called the interior of $S$. </latex>
<latex> Definition (15.5.3): Boundary of a Set </latex>	<latex> A point $\overrightarrow{x_{0}}$ is said to be a boundary point of the set $S$ if every neighborhoodof $\overrightarrow{x_{0}}$ contains points that are in $S$ and points that are not in $S$. The set of allboundary points of $S$ is called the boundary of $S$. </latex>
<latex> Definition (15.5.4): Open Set </latex>	<latex> A set $S$ is said to be open if it contains a neighborhood of each of its points. </latex>
<latex> Definition (15.5.5): Closed Set </latex>	<latex> A set $S$ is said to be closed if it contains its boundary. </latex>
<latex> Definition (15.6.1): Limit of a Function of Several Veriables </latex>	<latex> Let $f$ be a function defined at least on some deleted neighborhood of $\overrightarrow{x_{0}}$.$$\lim_{\overrightarrow{x}\to\overrightarrow{x_{0}}} f(\overrightarrow{x}) = L$$provided that for each $\epsilon > 0$ there exists a $\delta > 0$ such that$$ \text{if } 0 < ||\overrightarrow{x} - \overrightarrow{x_{0}}|| < \delta \text{, then } |f(\overrightarrow{x}) - L| < \epsilon \text{.}$$ </latex>
<latex> Definition (15.6.2): Continuity </latex>	<latex> $$\lim_{\overrightarrow{x} \to \overrightarrow {x_{0}}} f(\overrightarrow{x})=\overrightarrow{x_{0}}$$ </latex>
<latex> Definition (15.6.3): Continuity (in terms of $h \to 0$) </latex>	<latex> $$\lim_{\overrightarrow{h} \to \overrightarrow {0}} f(\overrightarrow{x} + \overrightarrow{h})=\overrightarrow{x_{0}}$$ </latex>
<latex> Theorem (15.6.4): The Continuity of Composite Functions </latex>	<latex> If $g$ is continuous at the point $x_{0}$ and $f$ is continuous at the number $g(x_{0})$, thenthe composition $f \circ g$ is continuous at the point $x_{0}$. </latex>
<latex> Theorem (15.6.5): Clairaut's Theorem (Symmetry of Second Partial Derivitives) </latex>	<latex> $$\frac{\partial^{2} f }{\partial y \partial x} = \frac{\partial^{2} f}{\partial x \partial y}$$ on every open set on which $f$ and the partials$$ \frac{\partial f}{\partial x} \text{,} \frac{\partial f}{\partial y} \text{,} \frac{\partial^{2} f}{\partial y \partial x} \text{,} \frac{\partial^{2} f}{\partial x \partial y}$$are continuous. </latex>
<latex> Definition (16.1.1): Differentiability of Vector Equations </latex>	<latex> We say that $f$ is differentiable at $x$ provided that there exists a vector $\overrightarrow{y}$ suchthat $$f(\overrightarrow{x} + \overrightarrow{h}) - f(\overrightarrow{x}) = \overrightarrow{y} \cdot \overrightarrow{h} + o(\overrightarrow{h})$$. </latex>
<latex> Definition (16.1.2): Gradient </latex>	<latex> Let $f$ be differentiable at $\overrightarrow{x}$. The gradient of $f$ at $x$ is the unique vector $\bigtriangledown f(x)$such that$$ f(\overrightarrow{x} + \overrightarrow{h}) - f(\overrightarrow{x}) = \bigtriangledown f(\overrightarrow{x}) \cdot \overrightarrow{h} + o(\overrightarrow{h})$$.  </latex>
<latex> Theorem (16.1.3): Formula for the Gradient </latex>	<latex> If $f$ has continuous first partials in a neighborhood of $x$, then $f$ is differentiable at $x$ and$$ \bigtriangledown f(\overrightarrow{x}) = \frac{\partial f}{\partial x}(\overrightarrow{x}) \overrightarrow{i} + \frac{\partial f}{\partial y}(\overrightarrow{x}) \overrightarrow{j} + \frac{\partial f}{\partial z}(\overrightarrow{x}) \overrightarrow{k}$$And for two variables: $$ \bigtriangledown f(\overrightarrow{x}) = \frac{\partial f}{\partial x}(\overrightarrow{x}) \overrightarrow{i} + \frac{\partial f}{\partial y}(\overrightarrow{x}) \overrightarrow{j} +$$ </latex>
<latex> Theorem (16.1.6) Differentiability Implies Continuity </latex>	<latex> If $f$ is differentiable at $x$, then $f$ is continuous at $\overrightarrow{x}$. </latex>
<latex> Theorem (16.1.4): Gradient of $r$, where $r = ||\overrightarrrow{r}||$ </latex>	<latex> If $r = ||\overrightarrrow{r}||$ and $\overrightarrrow{r} = x\overrightarrow{i} + y\overrightarrow{j} + z\overrightarrow{k}$, then if $r \neq 0$, $$\bigtriangledown r = \frac{\overrightarrow{r}}{r} \text{ and } \bigtriangledown (\frac{1}{r}) = - \frac{\overrightarrow{r}}{r^{3}}$$ </latex>
<latex> Theorem (16.1.5): Gradient of $r^{n}$, where $r = ||\overrightarrrow{r}||$ </latex>	<latex> $\bigtriangledown r^{n} = nr^{n-2}\overrightarrow{r}$ </latex>
<latex> Theorem (16.2.1): Properties of Gradients of Multiple Functions </latex>	<latex> $$\bigtriangledown [f(\overrightarrow{x}) + g(\overrightarrow{x})] = \bigtriangledown f(\overrightarrow{x}) + \bigtriangledown  g(\overrightarrow{x})$$$$\bigtriangledown [\alpha f(\overrightarrow{x})] = \alpha \bigtriangledown f(\overrightarrow{x})$$$$\bigtriangledown [f(\overrightarrow{x}) g(\overrightarrow{x})] = f(\overrightarrow{x})\bigtriangledown g(\overrightarrow{x}) + g(\overrightarrow{x})\bigtriangledown  f(\overrightarrow{x})$$ </latex>
<latex> Definition (16.2.2): Directional Derivative </latex>	<latex> For each unit vector $\overrightarrow{u}$, the limit$$f'_{u}(\overrightarrow{x}) = \lim_{h \to 0} \frac{f(\overrightarrow{x} + h \overrightarrow{u}) - f(\overrightarrow{x})}{h}$$if it exists, is called the directional derivative of $f$ at $\overrightarrow{x}$ in the direction $f(\overrightarrow{u}$. </latex>
<latex> Theorem (16.2.3): Partial Derivatives are Directional Derivatives </latex>	<latex> $$\frac{\partial f}{\partial x}(\overrightarrow{x}) = f'_{\overrightarrow{i}}(\overrightarrow{x}) \text{, } \frac{\partial f}{\partial y}(\overrightarrow{x}) = f'_{\overrightarrow{j}}(\overrightarrow{x}) \text{, } \frac{\partial f}{\partial z}(\overrightarrow{x}) = f'_{\overrightarrow{k}}(\overrightarrow{x})$$ </latex>
<latex> Theorem (16.2.4): The Directional Derivitive in Relation to the Gradient </latex>	<latex> If $f$ is differentiable at $\overrightarrow{x}$, then $f$ has a directional derivative at $\overrightarrow{x}$ in everydirection, and for each unit vector $\overrightarrow{u}$$$f'_{\overrightarrow{u}}(\overrightarrow{x}) = \bigtriangledown f(\overrightarrow{x}) \cdot \overrightarrow{u}$$ </latex>
<latex> Theorem (16.2.5): Existence of the Gradient if Differentiable </latex>	<latex> If $f$ is differentiable at $\overrightarrow{x}$, then all the first partial derivitives of $f$ exist at $\overrightarrow{x}$ and$$ \bigtriangledown f(\overrightarrow{x}) = \frac{\partial f}{\partial x}(\overrightarrow{x}) \overrightarrow{i} + \frac{\partial f}{\partial y}(\overrightarrow{x}) \overrightarrow{j} + \frac{\partial f}{\partial z}(\overrightarrow{x}) \overrightarrow{k}$$And for two variables: $$ \bigtriangledown f(\overrightarrow{x}) = \frac{\partial f}{\partial x}(\overrightarrow{x}) \overrightarrow{i} + \frac{\partial f}{\partial y}(\overrightarrow{x}) \overrightarrow{j} +$$ </latex>
<latex> Theorem (16.2.6): The Directional Derivitive in Relation to the Compliment of the Gradient onto the Unit Vector </latex>	<latex> $$f'_{\overrightarrow{u}}(\overrightarrow{x}) = comp_{\overrightarrow{u}} \bigtriangledown f(\overrightarrow{x})$$ </latex>
<latex> Theorem (16.2.7): Implication of the Gradient on the Rate of Change </latex>	<latex> from each point $\overrightarrow{x}$ of the domain, a differentiable function $f$ increasesmost rapidly in the direction of the gradient (the rate of change at $\overrightarrow{x}$being $||\bigtriangledown f(\overrightarrow{x})||$) the function decreases most rapidly in the oppositedirection (the rate of change at $\overrightarrow{x}$ being $-||\bigtriangledown f(\overrightarrow{x})||$). </latex>
<latex> Theorem (16.3.1): The Mean Value Theorem (Several Variables) </latex>	<latex> If $f$ is differentiable at each point of the line segment $\overline{ab}$, then there exists on that line segment a point $\overrightarrow{c}$ between $\overrightarrow{a}$ and $\overrightarrow{b}$ such that$$f(\overrightarrow{b}) - f(\overrightarrow{a}) = \bigtriangledown f(\overrightarrow{c}) \cdot (\overrightarrow{b} - \overrightarrow{a})$$ </latex>
<latex> Theorem (16.3.2): Constant Functions of Several Variables </latex>	<latex> Let $U$ be an open connected set and let $f$ be a differentiable function on $U$.If $\bigtriangledown f(x) = \overrightarrow{0}$ for all $\overrightarrow{x}$ in $U$, then $f$ is constant on $U$. </latex>
<latex> Theorem (16.3.3): Functions of Several Variables with Equivalent Gradients </latex>	<latex> Let $U$ be an open connected set and let $f$ and $g$ be functions differentiableon $U$.If $\bigtriangledown f(\overrightarrow{x}) = \bigtriangledown g(\overrightarrow{x})$ for all $x$ in $U$, then $f$ and $g$ differ by a constant on $U$. </latex>
<latex> Theorem (16.3.4): The Chain Rule for Functions of Several Variables </latex>	<latex> If $f$ is continuously differentiable on an open set $U$ and $\overrightarrow{r} = \overrightarrow{r}(t)$ is adifferentiable curve that lies in $U$, then the composition $f \circ \overrightarrow{r}$ is differentiable and$$\frac{d}{dt}[f(\overrightarrow{r}(t))] = \bigtriangledown f(\overrightarrow{r}(t)) \cdot \overrightarrow{r}'(t)$$ </latex>
<latex> Theorem (16.3.5): The Chain Rule for Functions of Three Variables </latex>	<latex> $$\frac{d}{dt}[u(\overrightarrow{r}(t))] = \bigtriangledown u(\overrightarrow{r}(t)) \cdot \overrightarrow{r}'(t)$$can be rewritten as$$\frac{du}{dt} = \frac{\partial u}{\partial x} \frac{dx}{dt} + \frac{\partial u}{\partial y} \frac{dy}{dt} + \frac{\partial u}{\partial z} \frac{dz}{dt}$$ </latex>
<latex> Theorem (16.3.6): The Chain Rule for Functions of Two Variables </latex>	<latex> $$\frac{d}{dt}[u(\overrightarrow{r}(t))] = \bigtriangledown u(\overrightarrow{r}(t)) \cdot \overrightarrow{r}'(t)$$can be rewritten as$$\frac{du}{dt} = \frac{\partial u}{\partial x} \frac{dx}{dt} + \frac{\partial u}{\partial y} \frac{dy}{dt}$$ </latex>
<latex> Theorem (16.3.7): Chain Rule of a Function of Two Functions of Two Variables </latex>	<latex> $$\text{if } u = u(x, y) \text{ and } x = x(s, t) \text{ and } y = y(s, t)$$then$$\frac{\partial u}{\partial s} = \frac{\partial u}{\partial x} \frac{\partial x}{\partial s} + \frac{\partial u}{\partial y} \frac{\partial y}{\partial s} \text{ and } \frac{\partial u}{\partial t} = \frac{\partial u}{\partial x} \frac{\partial x}{\partial t} + \frac{\partial u}{\partial y} \frac{\partial y}{\partial t}$$ </latex>
<latex> Theorem (16.3.8): Chain Rule on a Function of Three Functions of Two Variables </latex>	<latex> $$\text{if } u = u(x, y) \text{ and } x = x(s, t) \text{ and } y = y(s, t) \text{ and } z = z(s,t)$$then$$\frac{\partial u}{\partial s} = \frac{\partial u}{\partial x} \frac{\partial x}{\partial s} + \frac{\partial u}{\partial y} \frac{\partial y}{\partial s} + \frac{\partial u}{\partial z} \frac{\partial z}{\partial s} \text{ and } \frac{\partial u}{\partial t} = \frac{\partial u}{\partial x} \frac{\partial x}{\partial t} + \frac{\partial u}{\partial y} \frac{\partial y}{\partial t} + \frac{\partial u}{\partial z} \frac{\partial z}{\partial t}$$ </latex>
<latex> Theorem (16.3.9): (?) </latex>	<latex> If $u = u(x,y)$ is continuously differentiable, and $y$ is a differentiablefunction of $x$ that satisfies the equation $u(x,y) = 0$, then at all points$x, y$ where $\partial u / \partial y \neq 0$,$$ \frac{dy}{dx} = -\frac{\partial u / \partial x}{\partial u / \partial y}$$ </latex>
<latex> Theorem (16.3.10): (?) </latex>	<latex> If $u = u(x,y, z)$ is continuously differentiable, and $z = z(x, y)$ is a differentiablefunction that satisfies the equation $u(x,y, z) = 0$, then at all points$(x, y, z)$ where $\partial u / \partial z \neq 0$,$$ \frac{\partial z}{\partial x} = -\frac{\partial u / \partial x}{\partial u / \partial z} \text{ and } \frac{\partial z}{\partial y} = -\frac{\partial u / \partial y}{\partial u / \partial z}$$ </latex>
<latex> Theorem (16.4.1): The Gradient Vector is Related to the Level Curve </latex>	<latex> At each point of the domain, the gradient vector $\bigtriangledown f$, if not $\overrightarrow{0}$,is perpendicular to the level curve of $f$ that passes throughthat point. </latex>
<latex> Theorem (16.4.2): The Gradient Evaluated at a Particular Point in Relation to the Partials </latex>	<latex> $$\bigtriangledown f(x_{0}, y_{0}) = \frac{\partial f}{\partial x}(x_{0}, y_{0})\overrightarrow{i} + \frac{\partial f}{\partial y}(x_{0}, y_{0})\overrightarrow{j} </latex>
<latex> Theorem (16.4.3): Equation for the Normal Vector </latex>	<latex> $$\overrightarrow{t}(x_{0}, y_{0}) = \frac{\partial f}{\partial y}(x_{0}, y_{0})\overrightarrow{i} - \frac{\partial f}{\partial x}(x_{0}, y_{0})\overrightarrow{j} </latex>
<latex> Theorem (16.4.4): Equation for the Tangent Line in Two Dimensions </latex>	<latex> A point $(x, y)$ will lie on the tangent line if and only if$$\frac{\partial f}{\partial x}(x_{0}, y_{0})(x-x_{0}) + \frac{\partial f}{\partial y}(x_{0}, y_{0})(y - y_{0}) = 0$$ </latex>
<latex> Theorem (16.4.5): Equation for the Normal Line </latex>	<latex> A point $(x, y)$ will lie on the normal line if and only if$$\frac{\partial f}{\partial y}(x_{0}, y_{0})(x-x_{0}) - \frac{\partial f}{\partial y}(x_{0}, y_{0})(y - y_{0}) = 0$$ </latex>
<latex> Theorem (16.4.6): The Gradient Vector in Relation to Level Surfaces </latex>	<latex> At each point of the domain, the gradient vector, if not $\overrightarrow{0}$, isperpendicular to the level surface that passes through that point. </latex>
<latex> Theorem (16.4.7): General Multivariable Equation for the Tangent Plane </latex>	<latex> A point $\overrightarrow{x}$ lies on the tangent plane through $\overrightarrow{x_{0}}$ if and only if$$\bigtriangledown f(\overrightarrow{x_{0}}) \cdot (\overrightarrow{x} - \overrightarrow{x_{0}}) = 0$$ </latex>
<latex> Theorem (16.4.8): Equation for the Tangent Plane in Three Dimensions </latex>	<latex> A point $(x, y, z)$ will lie on the tangent plane if and only if$$\frac{\partial f}{\partial x}(x_{0}, y_{0}, z_{0})(x-x_{0}) + \frac{\partial f}{\partial y}(x_{0}, y_{0}, z_{0})(y - y_{0}) + + \frac{\partial f}{\partial z}(x_{0}, y_{0}, z_{0})(z - z_{0}) = 0$$ </latex>
<latex> Theorem (16.4.9): General Multivariable Equation for the Normal Line </latex>	<latex> $\overrightarrow{r}(t) = \overrightarrow{r_{0}} + \bigtriangledown f(\overrightarrow{x_{0}})t \text{ and } \overrightarrow{r_{0}} = x_{0}\overrightarrow{i} + y_{0}\overrightarrow{j} + z_{0}\overrightarrow{k}$ </latex>
<latex> Theorem (16.4.10): Equations for the Normal Line in Scalar Parametric Form </latex>	<latex> $$x = x_{0} + \frac{\partial f}{\partial x}(x_{0}, y_{0}, z_{0})t$$$$y = y_{0} + \frac{\partial f}{\partial y}(x_{0}, y_{0}, z_{0})t$$$$z = z_{0} + \frac{\partial f}{\partial z}(x_{0}, y_{0}, z_{0})t$$ </latex>
<latex> Theorem (16.4.11): Equation for the Tangent Plane Rewritten in terms of $z - z_{0}$ </latex>	<latex> $z - z_{0} = \frac{\partial g}{\partial x}(x_{0}, y_{0})(x - x_{0}) + \frac{\partial g}{\partial y}(x_{0}, y_{0})(y - x_{0})$ </latex>
<latex> Theorem (16.4.12): Scalar Parametric Equations for the Line Normal to the Surface $z = g(x, y)$ at the point $(x_{0}, y_{0}, z_{0})$ </latex>	<latex> $$x = x_{0} + \frac{\partial g}{\partial x}(x_{0}, y_{0})t \text{ and } y = y_{0} + \frac{\partial g}{\partial y}(x_{0}, y_{0})t \text{ and } z = z_{0} + (-1)t </latex>
<latex> Definition (16.5.1): Local Extreme Values </latex>	<latex> Suppose that $f$ is a function of several variables and $x_{0}$ is an interior point ofthe domain.$\\$ The function $f$ is said to have a local maximum at $\overrightarrow{x_{0}}$ provided that$f(\overrightarrow{x_{0}}) \geq f(\overrightarrow{x})$ for all $\overrightarrow{x}$ in some neighborhood of $\overrightarrow{x}$ $\\$ $\\$ The function $f$ is said to have a local minimum at $\overrightarrow{x_{0}}$ provided that$f(\overrightarrow{x_{0}}) \leq f(\overrightarrow{x})$ for all $\overrightarrow{x}$ in some neighborhood of $\overrightarrow{x_{0}}$.The local maxima and minima of $f$ comprise the local extreme values of $f$. $\\$ </latex>
<latex> Theorem (16.5.2): Local Extreme Values and the Gradient </latex>	<latex> If $f$ has a local extreme value at $\overrightarrow{x_{0}}$, then$$\bigtriangledown f(\overrightarrow{x_{0}}) = 0 \text{ or } \bigtriangledown f(\overrightarrow{x_{0}}) \text{ does not exist.}$$ </latex>
<latex> Theorem (16.5.3): The Second Partials Test </latex>	<latex> Suppose that $f$ has continuous second-order partial derivatives in aneighborhood of $(x_{0}, y_{0})$ and $\bigtriangledown f(x_{0}, y_{0}) = 0$ Set$$ A = \frac{\partial^{2} f}{\partial x^{2}}(x_{0}, y_{0}) \text{   } B = \frac{\partial^{2} f}{\partial y \partial x}(x_{0}, y_{0}) \text{   } C = \frac{\partial^{2} f}{\partial y^{2}}(x_{0}, y_{0}) $$and form the discriminant $D = AC - B^{2}$$\\$ 1. If $D < 0$, then $(x_{0}, y_{0})$ is a saddle point.$\\$ 2. If $D > 0$, then $f$ has$\\$ a local minimum at $(x_{0}, y_{0})$ if $A>0$,$\\$ a local maximum at $(x_{0}, y_{0})$ if $A>0$. </latex>
<latex> Definition (16.6.1): Absolute Extreme Values </latex>	<latex> Suppose that $f$ is a function of several variables.$\\$ $f$ is said to have an absolute maximum at $\overrightarrow{x_{0}}$ provided that$\\$ $f(\overrightarrow{x_{0}}) \geq f(\overrightarrow{x}$ for all $\overrightarrow{x}$ in the domain of $f$ $\\$ $f$ is said to have an absolute minimum at $\overrightarrow{x_{0}}$ provided that$\\$ $f(\overrightarrow{x_{0}}) \leq f(\overrightarrow{x}$ for all $\overrightarrow{x}$ in the domain of $f$ </latex>
<latex> Definition (16.6.1): Bounded Subset </latex>	<latex> A subset $S$ of the plane or three-space is said to be bounded provided thereexists a positive number $R$ such that$$||X|| \leq R \text{ for all } \overrightarrow{x} \in S$$ </latex>
<latex> Theorem (16.6.3): The Extreme Value Theorem for Several Variables </latex>	<latex> If $f$ is continuous on a closed and bounded set $D$, then on that set $f$ takes onan absolute maximum and an absolute minimum. </latex>
<latex> Definition (17.1.1): Multiple Sigma Notation </latex>	<latex> $$\sum_{i=1}^{m} \sum_{j=1}^{n} a_{ij}$$means the sum of all the $a_{ij}$ where $i$ ranges from $1$ to $m$ and $j$ ranges from $1$ to $n$.  </latex>
<latex> Theorem (17.1.2): Multiple Sigma Notation Seperated </latex>	<latex> $$\sum_{i=1}^{m} \sum_{j=1}^{n} a_{ij} = \sum_{i=1}^{m} (\sum_{j=1}^{n} a_{ij}) $$ </latex>
<latex> Theorem (17.1.3): Factoring of Multiplication On Multiple Sigma Notations </latex>	<latex> $$ \sum_{i=1}^{m} \sum_{j=1}^{n} \alpha a_{ij} = \alpha \sum_{i=1}^{m} \sum_{j=1}^{n} a_{ij} $$ </latex>
<latex> Theorem (17.1.4): Factoring of Addition of Multiple Sigma Notations </latex>	<latex> $$ \sum_{i=1}^{m} \sum_{j=1}^{n} (a_{ij} + b_{ij}) = \sum_{i=1}^{m} \sum_{j=1}^{n} a_{ij} + \sum_{i=1}^{m} \sum_{j=1}^{n} b_{ij}$$ </latex>
<latex> Theorem (17.1.5): Double Sums Seperable into Two Seperate Single Sums </latex>	<latex> $$ \sum_{i=1}^{m} \sum_{j=1}^{n} (b_{i}c_{j}) = (\sum_{i=1}^{m} b_{i})(\sum_{j=1}^{n} c_{j})$$ </latex>
<latex> Definition (17.1.6): Triple Sigma Notation </latex>	<latex> $$\sum_{i=1}^{m} \sum_{j=1}^{n} \sum_{k=1}^{q} a_{ijk}$$means the sum of all the $a_{ij}$ where $i$ ranges from $1$ to $m$ and $j$ ranges from $1$ to $n$ and $k$ ranges from $1$ to $q$.  </latex>
<latex> Definition (17.2.1): Upper Sum of $f$ </latex>	<latex> $$U_{f}(P) = \sum_{i=1}^{m} \sum_{j=1}^{n} M_{ij} (\text{area of } R_{ij}) = \sum_{i=1}^{m} \sum_{j=1}^{n} M_{ij} \delta x_{i} \delta y_{j}$$where $M_{ij}$ is the maximum value of $f$ on the rectangle $R_{ij}$ </latex>
<latex> Definition (17.2.2): Lower Sum for $f$ </latex>	<latex> $$L_{f}(P) = \sum_{i=1}^{m} \sum_{j=1}^{n} m_{ij} (\text{area of } R_{ij}) = \sum_{i=1}^{m} \sum_{j=1}^{n} m_{ij} \delta x_{i} \delta y_{j}$$where $m_{ij}$ is the minimum value of $f$ on the rectangle $R_{ij}$ </latex>
<latex> Definition (17.2.3): The Double Integral Over a Rectangle $R$ </latex>	<latex> Let $f$ be continuous on a closed rectangle $R$. The unique number $I$ that satisfiesthe inequality$$L_{f}(P) \leq I \leq U_{f}(P) \text{ for all partitions } P \text{ of } R$$is called the double integral of $f$ over $R$, and is denoted by$$ \iint_{R} f(x, y) dxdy$$ </latex>
<latex> Theorem (17.2.9): Mean Value Condition </latex>	<latex> There is a point (x_{0},y_{0}) in \omega for which $$\iint_{\omega}f(x,y) dxdy = f(x_{0},y_{0}) (\text{area of }\omega)$$We then call f(x_{0},y_{0}) the average value of f on $\omega$ and we can say that $$\iint_{\omega}f(x,y) dxdy = (\text{the average value of } f $\text{ on } \omega)(\text{area of }\omega)$$ </latex>
<latex> Theorem (17.2.10): The Mean Value Theorem for Double Integrals </latex>	<latex> Let $f$ and $g$ be functions continuous on a basic region $\omega$. If $g$ is nonnegativeon $\omega$, then there exists a point $(x_{0},y_{0})$ in $\omega$ for which$$\iint_{\omega}f(x,y)g(x,y)dxdy = f(x_{0},y_{0})\iint_{\omega}g(x,y)dxdy$$We call $f(x_{0},y_{0})$ the g-weighted average of $f$ on $\omega$. </latex>
<latex> Theorem (17.3.1): The Evaluation of Double Integrals by Repeated Integrals (Projection Type 1 (Interval Over $x$)) </latex>	<latex> The projection of $\Omega$ onto the x-axis is closed interval $[a,b]$ and $\Omega$ consists of all points $(x, y)$ with $$ a \leq x \leq b \text{ and } \phi_{1} \leq y \leq \phi_{2} \text{, then }$$$$\iint_{\Omega} f(x, y) dxdy = \int_{a}^{b}(\int_{\phi_{1}(x)}^{\phi_{2}(x)} f(x, y)dy)dx$$ </latex>
<latex> Theorem (17.3.2): The Evaluation of Double Integrals by Repeated Integrals (Projection Type 1 (Interval Over $y$)) </latex>	<latex> The projection of $\Omega$ onto the y-axis is closed interval $[c,d]$ and $\Omega$ consists of all points $(x, y)$ with $$ c \leq y \leq d \text{ and } \psi_{1} \leq x \leq \psi_{2} \text{, then }$$$$\iint_{\Omega} f(x, y) dxdy = \int_{c}^{d}(\int_{\psi_{1}(y)}^{\psi_{2}(y)} f(x, y)dx)dy$$ </latex>
<latex> Theorem (17.3.3): Separated Variables Over a Rectangle </latex>	<latex> Let $R$ be a rectangle $a \leq x \leq b$, $c \leq y \leq d$. Then, if $f$ is continuous on $[a, b]$ and $g$ is continuous on $[c, d]$, then $$\iint_{R} f(x)g(x) dxdy = [\int_{a}^{b}f(x)dx][\int_{c}^{d}g(y)dy]$$ </latex>
<latex> Theorem (17.4.1): Double Integrals as the Limit of Sums </latex>	<latex> $$\iint_{\omega} f(x, y) dxdy = \lim_{\text{diam} \omega_{i} \to 0} \sum_{i = 1}^{n} f(x_{i}^{*}, y_{i}^{*})(\text{area of } \omega_{i})$$ </latex>
<latex> Theorem (17.4.2): Area of a Region in Polar Coordinates </latex>	<latex> $$\text{area of }\omega = \iint_{\Gamma} r drd\theta$$ </latex>
<latex> Theorem (17.4.3): The Double Integral of a Function in Cartesian Coordinates in Polar Coordinates </latex>	<latex> $\iint_{\omega} f(x, y) dxdy = \iint_{\Gamma} f( r cos(\theta), r sin(\theta))r dr d\theta$$ </latex>
<latex> Theorem (17.4.4): Special Integral $\int_{-\infty}^{\infty} e^{-x^{2}} dx$ </latex>	<latex> $\int_{-\infty}^{\infty} e^{-x^{2}} dx = \sqrt{\pi}$ </latex>
<latex> Definition (13.2.1): Vector Addition </latex>	<latex> For:$$\overrightarrow{a} = (a_{1}, a_{2}, a_{3}) \text{ and } \overrightarrow{b} = (b_{1}, b_{2}, b_{3})$$We define:$$\overrightarrow{a} + \overrightarrow{b} = (a_{1} +b_{1} , a_{2} + b_{2} , a_{3} + b_{3} )$$ </latex>
<latex> Definition (13.2.2): Vector Scalar Multiplication </latex>	<latex> For:$$\overrightarrow{a} = (a_{1}, a_{2}, a_{3}) \text{ and } \alpha \text{ real}$$we define$$\alpha \overrightarrow{a} = (\alpha a_{1}, \alpha a_{2}, \alpha a_{3})$$ </latex>
<latex> Definition (13.2.3): Norm of a Vector </latex>	<latex> $$|| \overrightarrow{a} || = \sqrt{a_{1}^{2} + a_{2}^{2} + a_{3}^{2}}$$ </latex>
<latex> Theorem (13.2.4): Properties of the Norm of a Vector </latex>	<latex> $$(1) ||\overrightarrow{a}|| \geq 0 \text{ and } ||\overrightarrow{a}|| = 0 \text{ iff } \overrightarrow{a} = \overrightarrow{0}$$$$(2) ||\alpha \overrightarrow{a}|| = |\alpha| ||\overrightarrow{a}||$$$$(3) ||\overrightarrow{a} + \overrightarrow{b}|| \leq ||\overrightarrow{a}|| + ||\overrightarrow{b}||$$ </latex>
<latex> Theorem (13.2.5): Scalar Multiple of a Vector </latex>	<latex> For $\overrightarrow{a} \neq \overrightarrow{0}$, the vector $\overrightarrow{b} = \alpha \overrightarrow{a}$ is the unique vecotr of length $|a| ||\overrightarrow{a}||$, which has the direction of $\overrightarrow{a}$ if $\alpha > 0$ and the opposite direction if $\alpha < 0$.  </latex>
<latex> Theorem (13.2.6): Parallel Vectors </latex>	<latex> For $\overrightarrow{a} \neq \overrightarrow{0}$, the vectors parallel to $\overrightarrow{a}$ are by defition scalar multiples of $\alpha \overrightarrow{a}$ </latex>
<latex> Definition (13.2.7): Vectors Parallel to $\overrightarrow{0}$ </latex>	<latex> All vectors are said to be parallel to $\overrightarrow{0}$. </latex>
<latex> Theorem (13.2.8): Parallel Linear Combinations  </latex>	<latex> If $\overrightarrow{a}$ and $\overrightarrow{b}$ are both parallel to $\overrightarrow{c}$, then every linear combination of $\alpha \overrightarrow{a} + \beta \overrightarrow{b}$ is also parallel to $\overrightarrow{c}$ </latex>
<latex> Theorem (13.2.9): Unit Vectors </latex>	<latex> For each nonzero vector $\overrightarrow{a}$ ther is a unique unit vector $\overrightarrow{u_{a}}$ which has the direction of $\overrightarrow{a}$. $$\overrightarrow{u_{a}} = \frac{1}{||\overrightarrow{a}||}\overrightarrow{a} = \frac{\overrightarrow{a}}{||\overrightarrow{a}||}$$ </latex>
<latex> Defiition (13.2.10): $\overrightarrow{i}$, $\overrightarrow{j}$ and $\overrightarrow{k}$ </latex>	<latex> If $\overrightarrow{a} = (a_{1}, a_{2}, a_{3})$ then $\overrightarrow{a} = \overrightarrow{i}a_{1} + \overrightarrow{j}a_{2} + \overrightarrow{k}a_{3}$ </latex>
<latex> Definition (13.3.1): The Dot Product </latex>	<latex> For vectors$$\overrightarrow{a} = \overrightarrow{i}a_{1} + \overrightarrow{j}a_{2} + \overrightarrow{k}a_{3} \text{ and } \overrightarrow{b} = \overrightarrow{i}b_{1} + \overrightarrow{j}b_{2} + \overrightarrow{k}b_{3}$$we define the dot product $\overrightarrow{a} \cdot \overrightarrow{b}$ by setting $$\overrightarrow{a} \cdot \overrightarrow{b} = a_{1}b_{1} + a_{2}b_{2} + a_{3}b_{3}$$ </latex>
<latex> Theorem (13.3.2): $\overrightarrow{a} \cdot \overrightarrow{a}$ </latex>	<latex> $$\overrightarrow{a} \cdot \overrightarrow{a} = ||\overrightarrow{a}||^{2}$$ </latex>
<latex> Theorem (13.3.3): Dot Product with the Zero Vector </latex>	<latex> $$\overrightarrow{a} \cdot \overrightarrow{0} = 0 \text{, and } \overrightarrow{0} \cdot \overrightarrow{a} = 0$$ </latex>
<latex> Theroem (13.3.4): Commutativity of the Dot Product </latex>	<latex> $\overrightarrow{a} \cdot \overrightarrow{b} = \overrightarrow{b} \cdot \overrightattow{a}$ </latex>
<latex> Theorem (13.3.5): Scalar Factoring of the Dot Product </latex>	<latex> $\alpha \overrightarrow{a} \cdot \beta \overrightarrow{b} = \alpha \beta (\overrightarrow{a} \cdot \overrightarrow{b})$ </latex>
<latex> Theorem (13.3.6): Distributive Laws of the Dot Product </latex>	<latex> $$\overrightarrow{a} \cdot (\overrightarrow{b} + \overrightarrow{c}) = \overrightarrow{a} \cdot \overrightarrow{b} + \overrightarrow{a} \cdot \overrightarrow{c}$$$$ (\overrightarrow{a} + \overrightarrow{b}) \cdot \overrightarrow{c}) = \overrightarrow{a} \cdot \overrightarrow{c} + \overrightarrow{b} \cdot \overrightarrow{c}$$ </latex>
<latex> Theorem (13.3.7): Geometric Interpretation of the Dot Product and Defition of $\theta$ in Relation to the Dot Product </latex>	<latex> $\overrightarrow{a} \cdot \overrightarrow{b} = ||overrightarrow{a}||||\overrightarrow{b}|| cos \theta$ </latex>
<latex> Theorem (13.3.8): Defition of Perpendicular in Relation to the Dot Product </latex>	<latex> $$\overrightarrow{a} \perp \overrightarrow{b} \text{ iff } \overrightarrow{a} \cdot \overrightarrow{b} = 0$$ </latex>
<latex> Theorem (13.3.9): $cos \theta$ in Relation to Unit Vectors </latex>	<latex> $$cos \theta = \overrightarrow{u_{a}} \cdot \overrightarrow{u_{b}}$$ </latex>
<latex> Theorem (13.3.10): Linear Combinations of Vectors Perpendicular to the Same Vector </latex>	<latex> If $\overrightarrow{a}$ and $\overrightarrow{b}$ are both perpendicular to $\overrightarrow{c}$, then every linear combination of $\alpha \overrightarrow{a} + \beta \overrightarrow{b}$ is also perpendicular to $\overrightarrow{c}$ </latex>
<latex> Theorem (13.3.11): Definition of the Projection </latex>	<latex> $\overrightarrow{\text{proj}_{b}a} = (\overrightarrow{a} \cdot \overrightarrow{u_{b}})\overrightarrow{u_{b}}$$\text{comp}_{\overrightarrow{b}}\overrightarrow{a} = \overrightarrow{a} \cdot \overrightarrow{u_{b}}$ </latex>
<latex> Theorem (13.3.18): Schwarz's Inequality </latex>	<latex> $|\overrightarrow{a} \cdot \overrightarrow{b}| \leq ||\overrightarrow{a}||||\overrightarrow{b}||$ </latex>
<latex> Definition (13.4.1): The Cross Product </latex>	<latex> If $\overrightarrow{a}$ and $\overrightarrow{b}$ are not parallel, then $\overrightarrow{a} \times \overrightarrow{b}$ is the vector with the following properties:$$(1) \overrightarrow{a} \times \overrightarrow{b} \text{ is perpendicular tot eh plane of } \overrightarrow{a} \text{ and } \overrightarrow{b}$$$$(2) \overrightarrow{a}, \overrightarrow{b}, \text{ and } \overrightarrow{a} \times \overrightarrow{b} \text{ form a right handed triple}$$$$|| \overrightarrow{a} \times \overrightarrow{b}|| = ||\overrightarrow{a}||||\overrightarrow{b}|| sin \theta \text{ where } \theta \text{ is the angle between \overrightarrow{a} \text{ and } \overrightarrow{b}$$If $\overrightarrow{a} \text{ and } \overrightarrow{b}$ are parallel then $\overrightarrow{a} \times \overrightarrow{b} = \overrightarrow{0}$ </latex>
<latex> Theorem (13.4.2): Definition of a Right Handed Triple </latex>	<latex> $\overrightarrow{a}, \overrightarrow{b}, \text{ and }\overrightarrow{c}$ form a right-handed triple iff $(\overrightarrow{a} \times \overrightarrow{b}) \cdot \overrightarrow{c} > 0$ </latex>
<latex> Theorem (13.4.3): The Cross Product is Anticommutative </latex>	<latex> $\overrightarrow{b} \times \overrightarrow{a} = -(\overrightarrow{a} \times \overrightarrow{b})$ </latex>
<latex> Theorem (13.4.4): Scalar Factoring with Cross Products </latex>	<latex> $\alpha \overrightarrow{a} \times \beta \overrightarrow{b} = \alpha \beta (\overrightarrow{a} \times \overrightarrow{b})$ </latex>
<latex> Theorem (13.4.5): Distributive Laws with Cross Products </latex>	<latex> $$\overrightarrow{a} \times (\overrightarrow{b} + \overrightarrow{c}) = (\overrightarrow{a} \times \overrightarrow{b}) + (\overrightarrow{a} \times \overrightarrow{c})$$$$(\overrightarrow{a} + \overrightarrow{b}) \times \overrightarrow{c} = (\overrightarrow{a} \times \overrightarrow{c}) + (\overrightarrow{b} \times \overrightarrow{c})$$ </latex>
<latex> Theorem (13.4.7): Distributive Property of Cross Products with Dot Products </latex>	<latex> $(\overrightarrow{a} \times \overrightarrow{b}) \cdot \overrightarrow{c} = (\overrightarrow{c} \times \overrightarrow{a}) \cdot \overrightarrow{b} = (\overrightarrow{b} \times \overrightarrow{c}) \cdot \overrightarrow{a}$ </latex>
<latex> Theorem (13.4.8): Cross Products of the Cartesian Unit Vectors </latex>	<latex> $\overrightarrow{i} \times \overrightarrow{j} = \overrightarrow{k}$$\overrightarrow{j} \times \overrightarrow{k} = \overrightarrow{i}$$\overrightarrow{k} \times \overrightarrow{i} = \overrightarrow{j}$ </latex>
<latex> Theorem (13.4.9): Cross Product Expansion </latex>	<latex> $\overrightarrow{a} \times \overrightarrow{b} = (a_{2}b_{3} - a_{3}b_{2}, a_{1}b_{3} - a_{3}b_{1}, a_{1}b_{2} - a_{2}b_{1})$ </latex>
<latex> Theorem (13.4.9): Cross Product in Matrix/Determinant </latex>	<latex> $$\overrightarrow{a} \times \overrightarrow{b} = \begin{vmatrix}\overrightarrow{i} & \overrightarrow{j} & \overrightarrow{k} \\a_{1} & a_{2} & a_{3} \\b_{1} & b_{2} & b_{3} \end{vmatrix}$$ </latex>
<latex> Theorem (13.4.10): $(\overrightarrow{a} \times \overrightarrow{b}) \cdot \overrightright{c}$ </latex>	<latex> $(\overrightarrow{a} \times \overrightarrow{b}) \cdot \overrightright{c} = \begin{vmatrix}a_{1} & a_{2} & a_{3} \\b_{1} & b_{2} & b_{3} c_{1} & c_{2} & c_{3} \end{vmatrix}$ </latex>
<latex> Theorem (13.4.11): Cross Product with Cross Product </latex>	<latex> $$\overrightarrow{a} \times (\overrightarrow{b} \times \overrightarrow{c}) = (\overrightarrow{a} \cdot \overrightarrow{c})\overrightarrow{b} - (\overrightarrow{a} \cdot \overrightarrow{b})\overrightarrow{c}$$$$(\overrightarrow{a} \times \overrightarrow{b}) \times \overrightarrow{c}) = (\overrightarrow{c} \cdot \overrightarrow{a})\overrightarrow{b} - (\overrightarrow{c} \cdot \overrightarrow{b})\overrightarrow{a}$$ </latex>
<latex> Theorem (13.4.12): Dot Product of Two Cross Products </latex>	<latex> $$(\overrightarrow{a} \times \overrightarrow{b}) \cdot (\overrightarrow{c} \times \overrightarrow{d}) = (\overrightarrow{a} \cdot \overrightarrow{c})(\overrightarrow{b} \cdot \overrightarrow{d}) - (\overrightarrow{a} \cdot \overrightarrow{d})(\overrightarrow{b} \cdot \overrightarrow{c})$$ </latex>
<latex> Defitinion (13.5.1): Parametrization of a Line </latex>	<latex> $$\overrightarrow{r(t)} = \overrightarrow{r_{0}} + t \overrightarrow{d} \text{ for } t \text{ real}$$ </latex>
<latex> Definition (13.5.3): Scalar Parametric Equations </latex>	<latex> $$x(t) = x_{0} + td_{1}$$$$y(t) = y_{0} + td_{2}$$$$z(t) = z_{0} + td_{3}$$ </latex>
<latex> Theorem (13.5.2): Definition Line Through the Point $P(x_{1}, x_{2}, x_{3})$ with the direction $\overrightarrow{d} = (d_{1}, d_{2}, d_{3})$ </latex>	<latex> <!--anki-->$$\overrightarrow{r(t)} = (x_{0} + td_{1})\overrightarrow{i}+ (y_{0} + td_{2})\overrightarrow{j}+ (z_{0} + td_{3})\overrightarrow{k}$$ </latex>
<latex> Theorem (13.5.4): Symmetric Form of Parametric Equations of a Line </latex>	<latex> $$\frac{x-x_{0}}{d_{1}} = \frac{y-y_{0}}{d_{2}} = \frac{z-z_{0}}{d_{3}}$$ </latex>
<latex> Theorem (13.5.6): Distance from a Point to a Line </latex>	<latex> If $P_{0}$ is a point on $l$, a line, and $\overrightarrow{d}$ is the direction vector of ${l}$, then, $$d(P_{1}, l) = \frac{||\overrightarrow{P_{0}P_{1}} \times \overrightarrow{d}||}{||\overrightarrow{d}||}$$ </latex>
<latex> Theorem (13.6.1): Scalar Equation of a Plane </latex>	<latex> If $\overrightarrow{N}$ is the normal vector (perpendicular to all vectors on the plane) and $\overrightarrow{N} = (A, B, C)$ and $\overrightarrow{P_{0}} = (x_{0}, y_{0}, z_{0})$ is a point on the plan then the equation of the plane is given by:$$A(x - x_{0}) + B(y - y_{0}) + c(z - z_{0}) = 0$$ </latex>
<latex> Theorem (13.6.2): Vector Equation for a Plane </latex>	<latex> If $\overrightarrow{N}$ is the normal vector to a plane, and $\overrightarrow{r_{0}}$ is on the plane, then the equation for the plane is given by:$$\overrightarrow{N} \cdot (\overrightarrow{r} - \overrightarrow{r_{0}}) = 0$$ </latex>
<latex> Theorem (13.6.5): Distance from a Point to a Plane </latex>	<latex> If there exists a plane $p : Ax + By + Cz + D = 0$ and a point $P_{0}(x_{0}, y_{0}, z_{0})$ which is not on the plane, then:$$d(P_{0}, p) = \frac{|Ax_{0} + By_{0} + Cz_{0} + D|}{\sqrt{A^{2}+B^{2}+C^{2}}}$$ </latex>
<latex> Definition (14.1.1): Limit of a Vector Function </latex>	<latex> $$\lim_{t \to t_{0}} \overrightarrow{f(t)} = \overrightarrow{L} \text{ provided that } \lim_{t \to t_{0}} ||\overrightarrow{f(t)} - \overrightarrow{L}|| = 0$$ </latex>
<latex> Theorem (14.1.2): Limit of a Vector Function  </latex>	<latex> $$\text{ if } \lim_{t \to t_{0}} \overrightarrow{f(t)} = \overrightarrow{L} \text{ then } \lim_{t \to t_{0}} ||\overrightarrow{f(t)}|| = ||\overrightarrow{L}||$$ </latex>
<latex> Theorem (14.1.3): Limit Rules </latex>	<latex> Let $\overrightarrow{f}$ and $\overrightarrow{g}$ be vector functions and let $u$ be a real-valued function. Suppose that as $t \to \t_{0}$, $$\overrightarrow{f(t)} \to \overrightarrow{L} \text{ and } \overrightarrow{g(t)} \to \overrightarrow{M} \text{ and } u(t) \to A$$Then$$\overrightarrow{f(t)} + \overrightarrow{g(t)} \to \overrightarrow{L} + \overrightarrow{M}$$$$\alpha \overrightarrow{f(t)} + \beta \overrightarrow{g(t)} \to \alpha \overrightarrow{L} + \beta \overrightarrow{M}$$$$u(t)\overrightarrow{f(t)} \to A \overrightarrow{L}$$$$\overrightarrow{f(t)} \cdot \overrightarrow{g(t)} \to \\overrightarrow{L} \cdot \overrightarrow{M}$$$$\overrightarrow{f(t)} \times \overrightarrow{g(t)} \to \\overrightarrow{L} \times \overrightarrow{M}$$ </latex>
<latex> Theorem (14.1.4): Evaluation of a Limit Component by Component </latex>	<latex> $$\lim_{t \to \t_{0} \overrightarrow{f(t)} = \overrightarrow{L} \text{ iff }$$$$\lim_{t \to t_{0}} f_{1}(t) = L_{1} \text{ and } \lim_{t \to t_{0}} f_{2}(t) = L_{2} \text{ and } \lim_{t \to t_{0}} f_{3}(t) = L_{3}$$ </latex>
<latex> Definition (14.1.5): Derivative of a Vector Function </latex>	<latex> The vector function $\overrightarrow{f}$ is said to be differentiable at $t$ provided that $$\lim_{h \to 0} \frac{\overrightarrow{f(t + h)} - \overrightarrow{f(t)}}{h} \text{ exists}$$If this limit exists, it is called the derivative of $\overrightarrow{f}$ at $t$ and is denoted by $\overrightarrow{f'(t)}$. </latex>
<latex> Theorem (14.1.6): The Derivative of Constant Vector Functions </latex>	<latex> Constant Vector Functions have a Derivative of $\overrightarrow{0}$ </latex>
<latex> Theorem (14.1.7): Derivative of Vector Function with Factorable Constant Differentiable Function </latex>	<latex> If $u(t)$ is differentiable, then functions of the form$$\overrightarrow{f(t)} = u(t)\overrightarrow{c}$$ have the derivative$$\overrightarrow{f'(t)} = u'(t)\overrightarrow{c}$$ </latex>
<latex> Theorem (14.1.8): Integration of Vector Functions Component by Component </latex>	<latex> $$\int_{a}^{b} \overrightarrow{f(t)} dt = (\int_{a}^{b} f_{1} dt)\overrightarrow{i} + (\int_{a}^{b} f_{2} dt)\overrightarrow{j} + (\int_{a}^{b} f_{3} dt)\overrightarrow{k}$$ </latex>
<latex> Theorem (14.1.9 to 14.1.12): Properties of the Integral of Vector Functions </latex>	<latex> $$\int_{a}^{b}[\overrightarrow{f(t)} + \overrightarrow{g(t)}] dt = \int_{a}^{b}\overrightarrow{f(t)}dt + \int_{a}^{b}\overrightarrow{g(t)}dt$$$$\int_{a}^{b}[\alpha \overrightarrow{f(t)}]dt = \alpha \int_{a}^{b}\overrightarrow{f(t)}dt \text{ for every scalar constant } \alpha$$$$\int_{a}^{b}[\overrightarrow{c} \cdot \overrightarrow{f(t)}]dt = \overrightarrow{c} \cdot (\int_{a}^{b}\overrightarrow{f(t)}dt) \text{ for every constant vector} \overrightarrow{c}$$$$||\int_{a}^{b}\overrightarrow{f(t)}dt|| \leq \int_{a}^{b}||\overrightarrow{f(t)}||dt$$ </latex>
<latex> Theorem (14.2.1): Properties of the Derivative of Vector Functions </latex>	<latex> If $\overrightarrow{f}$, $\overrightarrow{g}$, and $u$ are continuous on a common domain, then$$(1) \overrightarrow{(f+g)'(t)} = \overrightarrow{f'(t)} + \overrightarrow{g'(t)}$$$$(2) (\overrightarrow{(\alpha f)'(t)} = \alpha \overrightarrow{f'(t)} \text{ and } \overrightarrow(uc)'(t) = u'(t)\overrightarrow{c}$$$$(3) \overrightarrow{(uf)'(t)} = u(t)\overrightarrow{f'(t)} + u'(t)\overrightarrow{f(t)}$$$$(4) (\overrightarrow{f} \cdot \overrightarrow{g})'(t) = [\overrightarrow{f(t)} \cdot \overrightarrow{g'(t)}] + [\overrightarrow{f'(t)} \cdot \overrightarrow{g(t)]$$$$(5) (\overrightarrow{f} \times \overrightarrow{g})'(t) = [\overrightarrow{f(t)} \times \overrightarrow{g'(t)}] + [\overrightarrow{f'(t)} \times \overrightarrow{g(t)}]$$$$(6) \overrightarrow{(f \circ u)'(t)} = \overrightarrow{f'(u(t))}u'(t) = u'(t) \overrightarrow{f'(u(t))}$$ </latex>
<latex> Theorem (14.2.3): The Dot Product of a Vector Function and it's Derivative </latex>	<latex> If $\overrightarrow{r}$ is a differentiable vector function of $t$, then the function $r = ||\overrightarrow{r}||$ is differentiable where it is not zero and $$\overrightarrow{r} \cdot \frac{d\overrightarrow{r}}{dt} = r\frac{dr}{dt}$$ </latex>
<latex> Theorem (14.2.4): The Derivative of the Unit Vector Function </latex>	<latex> If $\overrightarrow{r}$ is a differentiable vector function of $t$, then where $r = ||\overrightarrow{r}|| \neq 0$$$\frac{d}{dt}(\frac{\overrightarrow{r}}{r} = \frac{1}{r^{3}}[(\overrightarrow{r} \times \frac{d\overrightarrow{r}}{dt} \times \overrightarrow{r}]$$ </latex>
<latex> Definition (14.3.1): Tangent Vector </latex>	<latex> Let$$ C: \overrightarrow{r(t)} = x(t)\overrightarrow{i} + y(t)\overrightarrow{j} + z(t)\overrightarrow{k}$$be a differentiable curve. The vector $\overrightarrow{r'(t)}$, if not $\overrightarrow{0}$, is said to be tangent to the curve $C$ at the point $P(x(t), y(t), z(t))$ </latex>
<latex> Theorem (14.3.2): The Deriviative of a Curve's Direction in Relation to Increasing $t$.  </latex>	<latex> $\overrightarrow{r'(t)} points in the direction imparted by increasing $t$.  </latex>
<latex> Theorem (14.3.3): Parametrization of a Tangent Line at a Point of a Curve </latex>	<latex> If $\overrightarrow{r'(t_{0})} \neq \overrightarrow{0}$, then $\overrightarrow{r'(t_{0})}$ is tangent to the curve at the tip of $\overrightarrow{r(t_{0})}$. The tangent line at this point can be parametized by setting$$\overrightarrow{R(u)} = \overrightarrow{r(t_{0})} + u \overrightarrow{r'(t_{0})}$$ </latex>
<latex> Theorem (14.3.4): Unit Tangent Vector </latex>	<latex> $$\overrightarrow{T(t)} = \frac{\overrightarrow{r'(t)}}{||\overrightarrow{r'(t)}||}$$ </latex>
<latex> Theorem (14.3.5): Principal Normal Vector </latex>	<latex> $$\overrightarrow{N(t)} = \frac{\overrightarrow{T'(t)}}{||\overrightarrow{T'(t)}||}$$ </latex>
<latex> Definition (14.4.1): Arc Length </latex>	<latex> $$L(C) = { \text{ the least upper bound of the set of all lengths of polygonal paths inscribed in C }}$$ </latex>
<latex> Theorem (14.4.2): Arc Length Formula </latex>	<latex> Let $C$ be the path traced out by a continuously differentiable vector function$$\overrightarrow{r} = \overrightarrow{r(t)}, t\in[a, b]$$The length of $C$ is given by the formula $$L(C) = \int_{a}^{b}||\overrightarrow{r'(t)}||dt$$  </latex>
<latex> Theorem (14.4.3): Parametrization by Arc Length </latex>	<latex> If $$C: \overrightarrow{r} = \overrightarrow{r(t)}, t\in[a, b]$$is continuously differentiable curve of length $L$ with nonzero tangent vector $\overrightarrow{r'(t)}$. The length of $C$ from $\overrightarrow{r(a)}$ to $\overrightarrow{r(t)}$ is $$s(t) = \int_{a}^{t} ||\overrightarrow{r'(u)}|| du$$Since $ds/dt = ||\overrightarrow{r'(t)}|| > 0$, the function $s = s(t)$ is a one-to-one increasing function. Thus, no two points of $C$ can lie at the same arc distance from $\overrightarrow{r(a)}$. It follows that for each $s \in [0, L]$, there is a unique point $\overrightarrow{R(s)}$ on $C$ at arc distance $s$ from $\overrightarrow{r(a)}$Then, the function$$\overrightarrow{R} = \overrightarrow{R(s)}, s\in[0, L]$$parametizes $C$ by arc length.  </latex>
<latex> Theorem (14.4.4): The Length of the Tangent Vector of a Curve Parametrized by Arc Length </latex>	<latex> If a curce is parametrized by arc length, then the tangent vector can change in direction but no in length: the tangent vector maintains a length of $1$.   </latex>
<latex> Theorem (14.4.5): Arc Length Parametrization and Meaning on Length </latex>	<latex> If the curve$$C: \overrightarrow{r} = \overrightarrow{r(t)}, t\in[0, b]$$ has tangent vector of constant length 1, then the parametrization is by arc lenfth and the length of the curve is $b$.  </latex>
<latex> Definition (14.5.1): Velocity and Acceleration </latex>	<latex> $\overrightarrow{r'(t)} = \overrightarrow{v(t)}$ and $\overrightarrow{r''(t)} = \overrightarrow{v'(t)} = \overrightarrow{a(t)}$ </latex>
<latex> Definition (14.5.2): Speed </latex>	<latex> $$||\overrightarrow{v(t)}|| = \text{ the speed at time } t$$ </latex>
<latex> Theorem (14.5.3): Curviture of a Curve </latex>	<latex> For a unit speed curve, the curviture at $\overrightarrow{R(s)} is $$\kappa = ||\overrightarrow{T'(s)}||$$where $\overrightarrow{T(s)}$ is the tangent vector of the curve.  </latex>
<latex> 1. Vector Spaces and The Axioms of Vector Space </latex>	<latex> A vector space is a collection of objects, called vectors, along wih two operations, addition of vectors and multiplication by a scalar such that the following properties hold:$$\text{1. Commutativity: } \overrightarrow{v} + \overrightarrow{w} = \overrightarrow{w} + \overrightarrow{v} \text{ for all } \overrightarrow{v}, \overrightarrow{w} \in V$$$$\text{2. Associativity: } (\overrightarrow{u} + \overrightarrow{v}) + \overrightarrow{w} = \overrightarrow{u} + (\overrightarrow{v} + \overrightarrow{w}) \text{ for all } \overrightarrow{u}, \overrightarrow{v}, \overrightarrow{w} \in V$$$$\text{3. Zero Vector: there exists a special vector, denoted by } \overrightarrow{0} \text{ such that } \overrightarrow{v} + \overrightarrow{0} = \overrightarrow{v} \text{ for all } \overrightarrow{v} \in V$$$$\text{4. Additive Inverse: For every vector } \overrightarrow{v} \in V \text{ there exists a vector } \overrightarrow{w} \in V \text{ such that } \overrightarrow{v} + \overrightarrow{w} = \overrightarrow{0}. \text{ Such additive inverse is usually denoted as } -\overrightarrow{v}$$$$\text{5. Multiplicative Identity: } 1\overrightarrow{v} = \overrightarrow{v} \text{ for all } \overrightarrow{v} \in V$$$$\text{6. Multiplicative Associativity: } (\alpha \beta)\overrightarrow{v} = \alpha (\beta \overrightarrow{v}) \text{ for all } \overrightarrow{v} \in V \text{ and all scalars } \alpha, \beta$$$$\text{7. } \alpha(\overrightarrow{u} + \overrightarrow{v}) = \alpha \overrightarrow{u} + \alpha \overrightarrow{v} \text{ for all } \overrightarrow{u}, \overrightarrow{v} \in V \text{ and all scalars } \alpha$$$$\text{8. } (\alpha + \beta) \overrightarrow{v} = \alpha \overrightarrow{v} + \beta \overrightarrow{v} \text{ for all } \overrightarrow{v} \in V \text{ and all scalars } \alpha, \beta$$ </latex>
<latex> 1.2 Matrix Notation </latex>	<latex> An $m \times n$ matrix is a rectangular array with $m$ rows and $n$ columns. Elements of the array are called entries of the matrix. A general way to write an $m \times n$ matrix is:$$A = (a_{j, k})^{m}_{j=1},^{n}_{k=1} = \begin{vmatrix}a_{1,1} & a_{1,2} & \cdots & a_{1,n} \\a_{2,1} & a_{2,2} & \cdots & a_{2,n} \\\vdots & \vdots & & \vdots \\a_{m,1} & a_{m, 2} & \cdots & a_{m,n}\end{vmatrix}$$ </latex>
<latex> 1.2 Transpose Matrix </latex>	<latex> Given a matrix $A$, its transpose $A^{T}$, is defined by transforming the rows of $A$ into the columns. $$(A^{T})_{j,k} = (A)_{k,j}$$meaning that the entry of $A^{T}$ in the row number $j$ and the column number $k$ equals the entry of $A$ in the row number $k$ and the column number $j$.  </latex>
<latex> 1.2.1: Definition: Basis </latex>	<latex> A system of vectors $\overrightarrow{v_{1}}$, $\overrightarrow{v_{2}}$, ..., $\overrightarrow{v_{p}} \in V$ is called a basis if any vector $\overrightarrow{v} \in V$ admits a unique representation as a linear combination of the vectors in this system. $$\overrightarrow{v} = \alpha_{1} \overrightarrow{v_{1}} + \alpha_{2} \overrightarrow{v_{2}} + ... + \alpha_{n} \overrightarrow{v_{n}} = \sum_{k=1}^{n}\alpha_{k} \overrightarrow{v_{k}}$$ </latex>
<latex> 1.2.4: Remark: Operating on Basis Vector's Coefficients as Column Vectors </latex>	<latex> If $\overrightarrow{v} = \sum_{k=1}^{n}\alpha_{k} \overrightarrow{v_{k}}$ and $\overrightarrow{w} = \sum_{k=1}^{n} \beta_{kl} \overrightarrow{v_{k}}$, then$$\overrightarrow{v} + \overrightarrow{w} = \sum_{k=1}^{n} \alpha_{k} \overrightarrow{v_{k}} + \sum_{k=1}^{n} \beta_{k} \overrightarrow{v_{k}} = \sum_{k=1}^{n} (\alpha_{k} + \beta_{k}) \overrightarrow{v_{k}}$$ </latex>
<latex> 1.2.5: Definition: Generating System </latex>	<latex> A system of vectors $\overrightarrow{v_{1}}, \overrightarrow{v_{2}}, ..., \overrightarrow{v_{p}} \in V$ is called a generating system (also a spanning system, or complete system) in $V$ if any vector $\overrightarrow{v} \in V$ admits representation as a linear combinastion of the components of the system. The only difference from the definition of a basis is that we do not assume that the representation is unique. $$\overrightarrow{v} = \alpha_{1} \overrightarrow{v_{1}} + \alpha_{2} \overrightarrow{v_{2}} + ... + \alpha_{p} \overrightarrow{v_{p}} = \sum_{k=1}^{p}\alpha_{k} \overrightarrow{v_{k}}$$ </latex>
<latex> 1.2: Definition: Trivial Linear Combination </latex>	<latex> A linear combination $\alpha_{1} \overrightarrow{v_{1}} + \alpha_{2} \overrightarrow{v_{2}} + ... + \alpha_{p} \overrightarrow{v_{p}} = \sum_{k=1}^{p}\alpha_{k} \overrightarrow{v_{k}}$ is called trvial if $\alpha_{k} = 0$ for all $k$. A trivial linear combination is always equal to $\overrightarrow{0}$.  </latex>
<latex> 1.2: Definition: Linearly Independent </latex>	<latex> A system of vectors $\alpha_{1} \overrightarrow{v_{1}}, \alpha_{2} \overrightarrow{v_{2}}, ..., \alpha_{p} \overrightarrow{v_{p}} \in V$ is called linearly independent if the only trivial linear combination of vectors in the system is eqaul to $\overrightarrow{0}$. In other words$$x_{1}\overrightarrow{1} + x_{2}\overrightarrow{2} + ... + x_{p}\overrightarrow{p} = \overrightarrow{0} \text{ has the only trivial solution } x_{1} = x_{2} = ... = x_{p} = 0$$ </latex>
<latex> 1.2: Definition: Linearly Dependent </latex>	<latex> A system of vectors $\alpha_{1} \overrightarrow{v_{1}}, \alpha_{2} \overrightarrow{v_{2}}, ..., \alpha_{p} \overrightarrow{v_{p}} \in V$ is called linearly dependent if $\overrightarrow{0}$ can be represented as a nontrivial linear combination, meaning that at least one of the coefficients, $\alpha_{k}$ is non-zero. In other words$$x_{1}\overrightarrow{1} + x_{2}\overrightarrow{2} + ... + x_{p}\overrightarrow{p} = \overrightarrow{0} \text{ has at least one non trivial solution, so} \sum_{k=1}^{p} |x_{k}| \neq 0$$ </latex>
<latex> 1.2.6: Proposition: Linearly Dependent Systems Represented as Linear Combinations </latex>	<latex> A system of vectors $\alpha_{1} \overrightarrow{v_{1}}, \alpha_{2} \overrightarrow{v_{2}}, ..., \alpha_{p} \overrightarrow{v_{p}} \in V$ is linearly dependent if and only if one of the vectors $\overrightarrow{v_{k}}$ can be represented as a linear combination of the other vectors.  </latex>
<latex> 1.2.7: Proposition: A Basis is Linearly Independent and Generating </latex>	<latex> A system of vectors $\alpha_{1} \overrightarrow{v_{1}}, \alpha_{2} \overrightarrow{v_{2}}, ..., \alpha_{p} \overrightarrow{v_{p}} \in V$ is a basis if and only if it is linearly independent and complete.  </latex>
<latex> 1.2.8: Proposition: Generating Systems Contain a Basis </latex>	<latex> Any (finite) generating system contains a basis.  </latex>
<latex> 1.3.1: Defition: Linear Transformation </latex>	<latex> Let $V, W$ be vector spaces  (over the same field $\mathbb{F}$). A transformation $T: V \to W$ is called linear if:$$\text{1. } T(\overrightarrow{u} + \overrightarrow{v}) = T(\overrightarrow{u}) + T(\overrightarrow{v}) \text{ for all } \overrightarrow{u}, \overrightarrow{v} \in V$$$$\text{2. } T(\alpha \overrightarrow{v}) = \alpha T(\overrightarrow{v}) \text{ for all } \overrightarrow{v} \in V \text{ and for all scalars } \alpha \in \mathbb{F}$$These two properties can be combined in the equivalent statement:$$T(\alpha \overrightarrow{u} + \beta \overrightarrow{v}) = \alpha T(\overrightarrow{u}) + \beta T(\overrightarrow{v}) \text{ for all } \overrightarrow{u}, \overrightarrow{v} \in V \text{ and for all scalars } \alpha, \beta$$ </latex>
<latex> 1.3.2: Linear Transformations as Matrix Column Multiplication </latex>	<latex> Linear transformation $T: \mathbb{F}^{n} \to \mathbb{F}^{m}$ also can be represented as multiplication, not by a scalar, but  by a matrix. If $T(\overrightarrow{x}) = A\overrightarrow{x}$ where $A$ is some matrix. Then:$$A \overrightarrow{x} = \sum_{k=1}^{n} x_{k} \overrightarrow{a_{k}}$$Thus, the matrix-vector multiplication should be performed by multiplying each column of the matrix by the corresponding coordinate of the vector.  </latex>
<latex> 1.3.3: Linear Transformations and Generating Sets </latex>	<latex> A linear transformation $T: V \to W$ is completely defined by its values on a generating set (in particular by its values on a basis). $\\$ Then if $\alpha_{1} \overrightarrow{v_{1}}, \alpha_{2} \overrightarrow{v_{2}}, ..., \alpha_{p} \overrightarrow{v_{n}} \in V$ is a generating set (in particular, if it is a basis), and $T$ and $T_{1}$ are linear transformations $T, T_{1} : V \to W$ such that $$ T \overrightarrow{v_{k}} = T_{1} \overrightarrow{v_{k}}, k = 1, 2, 3, ..., n$$then $T = T_{1}$.  </latex>
<latex> 1.4.1: Remark: Linear Operations on Linear Transformations </latex>	<latex> Linear operations (addition and multipication by a scalar) on linear transformations $T: \mathbb{F}^n \to \mathbb{F}^m$ correspond to the respective operations on their matrices. Since we know that the set of $m \times n$ matrices is a vector space, this immediately impliers that $\mathcal{L}(\mathbb{F}^n, \mathbb{F}^m)$ is a vector space.  </latex>
<latex> 1.5.1 Definition: Matrix Multiplication </latex>	<latex> If $b_{1}, b_{2}, ... , b_{r}$ are the column vectors of $B$, then $Ab_{1}, Ab_{2}, ... , Ab_{r}$ are the columns of the matrix $AB$. $\\$ The entry $(AB)_{j,k}$ (the entry in row $j$ and column $k$) of the product $AB$ is defined by $$(AB)_{j, k} = (\text{row } j \text{ of } A) \cdot (\text{column} k \text{ of } B)$$$$(AB)_{j, k} = \sum_{l}a_{j, l}b_{l,k}$$ </latex>
<latex> 1.5.2: Definition: Composition of Linear Transformations </latex>	<latex> Suppose there are two linear transformations, $T_{1}: \mathbb{F}^n \to \mathbb{F}^m$ and $T_{2}: \mathbb{F}^r \to \mathbb{F}^n$. Then, the composition $T = T_{1} \circ T_{2}$ of the transformations $T_{1}, T_{2}$ is defined as:$$T(\overrightarrow{x}) = T_{1}(T_{2}(\overrightarrow{x})) \text{ for all } \overrightarrow{x} \in \mathbb{F}^r$$ </latex>
<latex> 1.5.3: Remark: Row by Column Rule of Composition Transformations </latex>	<latex> In order for $T_{1}T_{2}$ to be defined the matrices of $T_{1}$ and $T_{2}$ should be os sizes $m \times n$ and $n \times r$ respectively. </latex>
<latex> 1.5.3: Theorem: Properties of Matrix Multiplication </latex>	<latex> $$\text{1. Associativity } A(BC) = (AB) C \text{ provided that either left or right side is well define. Then typically written } ABC. $$$$\text{2. Distributivity } A(B+C) = AB + AC \text{ and } (A + B)C = AC + BC \text{ provided either left or right side of each quadrant is well defined. }$$$$\text{3. One can take scalar multiples out: } A(\alpha B) = (\alpha A)(B) = \alpha(AB) = \alpha AB$$ </latex>
<latex> 1.5.4: Theorem: Transposed Matrices and Multiplication </latex>	<latex> $$(AB)^{T} = B^{T}A^{T}$$ </latex>
<latex> 1.5.5: Definition: Trace and Matrix Multiplication </latex>	<latex> For a square ($n \times n$) matrix, $A = (a_{j, k})$, its trace (denoted by $\text{trace}A$) is the sum of the diagonal entries$$\text{trace}A = \sum_{k=1}{n}a_{k,k}$$ </latex>
<latex> 1.5.1: Theorem: Trace of Matrices Commutativity </latex>	<latex> Let $A$ and $B$ be matrices of size $m \times n$ and $n \times m$ respectively (so the both products $AB$ and $BA$ are well defined), then $$\text{trace}(AB) = \text{trace}(BA)$$ </latex>
<latex> 1.6.1: Definition: Identity Transformation and Identity Matrix </latex>	<latex> Among all linear transformations, there is a special identity transformation (operator), $I$, the identity transformation, or identity matrix such that $$I\overrightarrow{x} = \overrightarrow{x} \text{ for all } \overrightarrow{x}$$There ar infinitely many identity transformations: for any vector space $V$, there is the identity transformation $I = I_{v}: V \to V$ and $I_{v}\overrightarrow{x} = \overrightarrow{x}$ for all $\overrightarrow{x}$ in $V$.  </latex>
<latex> 1.6.2: Definition Left Invertible </latex>	<latex> Let $A: V \to W$ be a linear transformation. The transformation $A$ is left invertible is there exists a linear transformation $B: W \to V$ such that $$BA = I (I = I_{V})$$Then $B$ is the left inverse of $A$ </latex>
<latex> 1.6.2: Definition: Right Invertible </latex>	<latex> Let $A: V \to W$ be a linear transformation. The transformation $A$ is right invertible if there exists a linear transformation $C: W \to V$ such that $$AC = I (I = I_{W})$$Then $C$ is the right inverse of $A$ </latex>
<latex> 1.6.2: Definition: Invertible Transformations </latex>	<latex> A linear transformation $A: V \to W$ is called invertible if it is both right and left invertible.  </latex>
<latex> 1.6.2: Theorem 6.1: Uniqueness of Left and Right Inverses of an Invertible Transformation </latex>	<latex> If a linear transformation $A: V \to W$ is invertible, then its left and right inverses $B$ and $C$ are unique and coincide.  </latex>
<latex> 1.6.2: Theorem 6.1 Corollary: The Inverse of a Transformation </latex>	<latex> A transformation $A: V \to W$ is invertible if and only if there exists a unique linear transformation (denoted $A^{-1}$), $A^{-1} : W \to V$ such that $$A^{-1}A = I_{V} \text{ and } AA^{-1} = I_{W}$$Then the transformation $A^{-1}$ is called the inverse of $A$.  </latex>
<latex> 1.6.2: Definition: Invertible Matrices </latex>	<latex> A matrix is called invertible (respectively left invertible and right invertible) if the corresponding linear transformation is invertible (respectively left invertible and right invertible).  </latex>
<latex> 1.6.2: Remark: Squareness of Invertible Matrices and Right and Left Existence Sufficiency </latex>	<latex> An invertible matrix must be square ($n \times n$). Moreover, if a square matrix $A$ has either left or right inverse, it is invertible. So, it is sufficient to check only one of the indentites $AA^{-1} = I \text{ or } A^{-1}A = I$.  </latex>
<latex> 1.6.2: Theorem 6.3: Inverse of the Product </latex>	<latex> If linear transformations $A$ and $B$ are invertible (and such that the product $AB$ is defined), then the product $AB$ is invertible and $$(AB)^{-1} = B^{-1}A^{-1}$$Note the change of order.  </latex>
<latex> 1.6.2: Remark 6.4: Invertibility of the Factors of an Invertible Product </latex>	<latex> The inveribility of the product $AB$ does not imply the invertibility of the factors $A$ and $B$. However, if one of the factors $A$ or $B$ is invertible and $AB$ is invertible, then the second factor is also invertible.  </latex>
<latex> 1.6.2: Theorem 6.5: Inverse of $A^{T}$ </latex>	<latex> If a matrixc $A$ is invertible, then $A^{T}$ is is also invertible and $$(A^{T})^{-1} = (A^{-1})^{T}$$ </latex>
<latex> 1.6.3: Definition: Isomorphism </latex>	<latex> An invertible linear transformation $A: V \to W$ is called an isomorphism.  </latex>
<latex> 1.6.3: Definition: Isomorphic Spaces </latex>	<latex> Two vector spaces $V$ and $W$ are called isomorphic (denoted $V \cong W$) if there is an isomorphism $A: V \to W$.  </latex>
<latex> 1.6.3: Theorem 6.6: The Isomorphimic Transformation of a Basis </latex>	<latex> Let $A:V \to W$ be an isomorphism, and let $\overrightarrow{v_{1}}, \overrightarrow{v_{2}}, ... , \overrightarrow{v_{n}}$ be a basis in $V$. Then the system $A \overrightarrow{v_{1}}, A \overrightarrow{v_{2}}, ... , A \overrightarrow{v_{n}}$ is also a basis in $W$.  </latex>
<latex> 1.6.3: Remark, Theorem 6.6: Properties Preserved by Isomorphistic Transformations </latex>	<latex> Just as bases (plural of basis) are preserved by isomorphic transformations, so is linear independence, generation, and linear dependence.  </latex>
<latex> 1.6.3: Remark, Theorem 6.6: The Inverse of an Isomorphism </latex>	<latex> If $A$ is an Isomorphism, then so is $A^{-1}$. </latex>
<latex> 1.6.3: Theorem 6.7: Linear Transformation that Transforms from a Basis to Another Basis </latex>	<latex> Let $A: V \to W$ be a linear map, and let $\overrightarrow{v_{1}}, \overrightarrow{v_{2}}, ... , \overrightarrow{v_{n}}$ and $\overrightarrow{w_{1}}, \overrightarrow{w_{2}}, ... , \overrightarrow{w_{n}}$ be bases in $V$ and $W$ respectively. If $A\overrightarrow{v_{1}} = \overrightarrow{w_{k}} \text{ for all } k = 1, 2, ... , n$, then $A$ is an isomorphism.  </latex>
<latex> 1.6.4: Theorem 6.8: Equation that Guarentee's Invertibility </latex>	<latex> Let $A: X \to Y$ be a linear transformation. Then $A$ is invertible if and only if for any right side $\overrightarrow{b} \in Y$ the equation$$A\overrightarrow{x} = \overrightarrow{b}$$has the unique solution $\overrightarrow{x} \in X$. </latex>
<latex> 1.6.4: Corollary 6.9: The Columns of an Invertible Matrix </latex>	<latex> An $m \times n$ matrix is invertible if and only if its columns form a basis in $\mathbb{F}^m$.  </latex>
<latex> 1.7.1: Definition: Subspace </latex>	<latex> A subspace of a vector space $V$ is a non-empty subset $V_{0} \subset V$ of $V$ which is closed under the vector addition and multiplication by scalars, i.e.$$\text{1. If} \overrightarrow{v} \in V_{0} \text{ then } \alpha \overrightarrow{v} \in V_{0} \text{ for all scalars}$$$$\text{2. For any } \overrightarrow{u}, \overrightarrow{v} \in V_{0} \text{, the sum } \overrightarrow{u} + \overrightarrow{v} \in V_{0}$$Again, the conditions 1 and 2 can be replaced by the following one:$$\alpha \overrightarrow{u} + \beta \overrightarrow{v} \in V_{0} \text{ for any } \overrightarrow{u}, \overrightarrow{v} \in V_{0} \text{ and for all scalars}$$ </latex>
<latex> 2.1.1: Definition: Linear System </latex>	<latex> A system of linear equations, which can be written in matrix form (as a matrix vector equaition).  </latex>
<latex> 2.2.1: Definition: Row Operations </latex>	<latex> $$\text{1. Row excahnge: interchange two rows of the matrix}$$$$\text{2. Scaling: multiple a row by a non-zero scalar} \alpha$$$$\text{3. Row replacement: replace a row number } k \text{ by its sum with a constant multiple of a row number, } j \text{ all other rows remain intact}$$ </latex>
<latex> 2.2.2: Definition: Row Reduction </latex>	<latex> The three sub-steps of row reduction$$\text{1. Find the leftmost non-zero column of the matrix}$$$$\text{2. Make sure, by applying row operations of type 1 row exchange, ifnecessary, that the 1st the upper entry of this column is non-zero.This entry will be called the pivot entry or simply the pivot}$$$$\text{3. Kill i.e. make them 0 all non-zero entries below the pivot by adding or subtracting an appropriate multiple of the first row from the rows numbered } 2, 3, ..., m$$ </latex>
<latex> 2.2.3: Definition: Echelon Form </latex>	<latex> A matrix is in echelon form if it satises the following two conditions:$$\text{1. All zero rows (i.e. the rows with all entries equal 0), if any, are below all non-zero entries.}$$For a non-zero row, let us call the leftmost non-zero entry the leading entry.Then the second property of the echelon form can be formulated as follows:$$\text{2. For any non-zero row its leading entry is strictly to the right of the leading entry in the previous row.}$$ </latex>
<latex> 2.2.3: Definition: Reduced Echelon Form </latex>	<latex> A matrix is in reduced echelon form if it is in echelon form and:$$\text{3. All pivot entries are equal 1}$$$$\text{4. All entries above the pivots are 0. Note, that all entries below the pivots are also 0 because of the echelon form.}$$ </latex>
<latex> 2.3.1: Theorem: Inconsistient Linear Systems </latex>	<latex> A system is inconsistent (does not have a solution) if and only if there is a pivot in the last column of an echelon form of the augmented matrix. </latex>
<latex> 2.3.1: Properties of Coefficient Matrices Regarding Solutions </latex>	<latex> $$\text{ 1. A solution, if it exists, is unique if there are no free variables, that is if and only if there are pivots in every column of the echelon form of the coefficient matrix}$$$$\text{2. The equation } A \overrightarrow{x} = \overrightarrow{b} \text{is consistient for all right sides if and only the coefficient matrix has a pivot in every row}$$$$\text{3. The equation} A \overrightarrow{x} = \overrightarrow{b} \text{has a unique solution for the right side if and only if the echelon form for the coefficient matrix} A \text{ has a pivot in every column and every row}$$ </latex>
<latex> 2.3.1: Proposition 3.1: The Linear Dependence, Generation, and Bases of Vector Systems in Relation to Pivots in Echelon Form </latex>	<latex> Let us have a system of vectors $\overrightarrow{v_{1}}, \overrightarrow{v_{2}}, ... \overrightarrow{v_{m}} \in \mathbb{F}^n$, and let $A = [\overrightarrow{v_{1}}, \overrightarrow{v_{2}}, ... \overrightarrow{v_{m}}]$ be an $n \times m$ matrix with columns $\overrightarrow{v_{1}}, \overrightarrow{v_{2}}, ... \overrightarrow{v_{m}}$. Then, $$\text{1. The system } \overrightarrow{v_{1}}, \overrightarrow{v_{2}}, ... \overrightarrow{v_{m}} \text{ is linearly independing if and only if the echelon form of }A  \text{ has a pivot in every column}$$$$\text{2. The system } \overrightarrow{v_{1}}, \overrightarrow{v_{2}}, ... \overrightarrow{v_{m}} \text{ is complete in } \mathbb{F}^n \text{, thus spanning, generating, if and only if the echelon form of the matrix} A \text{ has a pivot in every row}$$$$\text{3. The system }\overrightarrow{v_{1}}, \overrightarrow{v_{2}}, ... \overrightarrow{v_{m}} \text{ is a basis in } \mathbb{F}^n \text{ if and only if echelon form of } A \text{ has a pivot in every column and in every row.}$$ </latex>
<latex> 2.3.1: Proposition 3.2 The Number of Vecotrs in a Linear Independent System of Vectors in $\mathbb{F}^n$ </latex>	<latex> Any linearly independent system of vectors in $\mathbb{F}^n$ cannothave more than $n$ vectors in it. </latex>
<latex> 2.3.1: Proposition 3.3: Bases in Vector Spaces Number of Vectors </latex>	<latex> Any two bases in a vector space $V$ have the same number of vectors in them. </latex>
<latex> 2.3.1: Proposition 3.4: Number of Vectors in a Basis </latex>	<latex> Any basis in $\mathbb{F}^n$ must have exactly $n$ vectors in it. </latex>
<latex> 2.3.1: Proposition 3.5: Number of Vectors in a Spanning or Generating Set </latex>	<latex> Any spanning (generating) set in $\mathbb{F}^n$ must have at least$n$ vectors. </latex>
<latex> 2.3.2: Proposition 3.6: Echelon Form Relation to Invertibility </latex>	<latex> A matrix $A$ is invertible if and only if its echelon form has pivot in every column and every row. </latex>
<latex> 2.3.2: Corollary 3.7: Shape of Invertible Matrices </latex>	<latex> An invertible matrix must be square ($n \times n$). </latex>
<latex> 2.3.2: Proposition 3.8: Implications of Right or Left Invertibility on a Square Matrix </latex>	<latex> If a square ($n \times n$) matrix is left invertible, or if it is right invertible, then it is invertible. In other words, to check the invertibility of a square matrix $A$ it is sufficient to check only one of the conditions.  </latex>
<latex> 2.4.1: Theorem: Finding $A^{-1}$ by Row Reduction </latex>	<latex> Any invertible matrix is row equivalent (i.e. can be reduced by rowoperations) to the identity matrix. </latex>
<latex> 2.4.1: THeorem 4.1: Invertible Matrix Representation with Elementary Matrices </latex>	<latex> Any invertible matrix can be represented as a product of elementary matrices. </latex>
<latex> 2.5.1: Definition: Dimension </latex>	<latex> The dimension dim$V$ of a vector space $V$ is the number of vectors in a basis. </latex>
<latex> 2.5.1: Proposition 5.1: The Spanning Systems of Finite Dimension Vector Spaces  </latex>	<latex> A vector space $V$ is finite-dimensional if and only if it has a finite spanning system. </latex>
<latex> 2.5.1: Proposition 5.2: Vectors in a Linear Independent System in a Finite Dimensional Vector Space </latex>	<latex> Any linearly independent system in a finite-dimensional vector space $V$ cannot have more than dim$V$ vectors in it. </latex>
<latex> 2.5.1: Proposition 5.3: Vectors in a Generating System in a Finite Dimensional Vector Space </latex>	<latex> Any generating system in a finite-dimensional vector space $V$ must have at least dim$V$ vectors in it. </latex>
<latex> 2.5.1: Proposition 5.4: Completion to a Basis </latex>	<latex> A linearly independent systemof vectors in a finite-dimensional space can be completed to a basis, i.e. if$\overrightarrow{v_{1}}, \overrightarrow{v_{2}}, ... , \overrightarrow{v_{r}}$ are linearly independent vectors in a finite-dimensional vector space $V$ then one can and vectors $\overrightarrow{v_{r+1}}, \overrightarrow{v_{r+2}}, ... , \overrightarrow{v_{n}}$ such that the system of vectors $\overrightarrow{v_{1}}, \overrightarrow{v_{2}}, ... , \overrightarrow{v_{n}}$ is a basis in $V$. </latex>
<latex> 2.6.1: Theorem 6.1: General Solution of a Linear Equation </latex>	<latex> Let a vector $\overrightarrow{x_{1}}$ satisfy the equation $A\overrightarrow{x} = \overrightarrow{b}$, and let $H$ be the set of all solutions of the associated homogeneous system.$$A\overrightarrow{x} = 0$$Then the set$${x = x_{1} + x_{h} \in H}$$is the set of all solutions of the equation $A\overrightarrow{x} = \overrightarrow{b}$. <br_off /> </latex>
<latex> 2.7.1: Definition: Rank </latex>	<latex> Given a linear transformation (matrix) $A$ its rank, $\text{rank}A$, is the dimension of the range of $A$$$\text{rank}A := \text{dim Ran}A$$ </latex>
<latex> 2.7.3: Theorem 7.1: The Rank Theorem </latex>	<latex> For the matrix $A$$$\text{rank}A = \text{rank} A^{T}$$The column rank of a matrix coincides with its row rank.  </latex>
<latex> 2.7.3: Theorem 7.2: Sum of Dimensions Equations </latex>	<latex> Let $A$ be an $m \times n$ matrix, i.e. a linear transformation from$\mathbb{F}^{n} \to \mathbb{F}^{m}$. Then$$\text{1. dim Ker}A+ \text{dim Ran}A = \text{dim Ker}A+\text{rank}A = n \text{(dimension of thedomain of }A\text{)}$$$$\text{2. dim Ker}A^{T} + \text{dim Ran}A^{T} = \text{dim Ker}A^{T} + \text{rank}A^{T} = m \text{(dimension of the target space of }A\text{)}$$ </latex>
<latex> 2.7.3: Theorem 7.3: Existence of a Unique Solution Based on the Daul's Equations Existence of a Solution </latex>	<latex> Let $A$ be an $m \times n$ matrix. Then the equation$$A\overrightarrow{x} = \overrightarrow{b}$$has a solution for every $\overrightarrow{b} \in \mathbb{R}^{m}$ if and only if the daul equation$$A^{T}\overrightarrow{x} = 0$$has a unique (only the trivial) solution. </latex>
<latex> 2.8.1: Definition 8.1: Similar Matrices </latex>	<latex> We say that a matrix $A$ is similar to a matrix $B$ if thereexists an invertible matrix $Q$ such that $A = Q^{-1}BQ$.  </latex>
<latex> 3.3.2: Proposition 3.1: Properties of Determinant Deducted from the Basic Properties </latex>	<latex> For a square matrix $A$ the following statements hold:$$\text{1. If } A \text{ has a zero column, det} A = 0$$$$\text{2. If } A \text{ has two equal columns, then det}A = 0$$$$\text{3. If one column of }A \text{ is a multiple of another, then det} A = 0$$$$\text{4. If columns of }A \text{are linearly dependent, i.e. if the matrix is not in-vertible, then det}A = 0$$ </latex>
<latex> 3.3.5: Propososition 3.2: Change in Determinant with Column Replacement </latex>	<latex> The determinant does not change if we add to a column a linear combination of the other columns (leaving the other columnsintact). In particular, the determinant is preserved under "column replace-ment" (column operation of third type). </latex>
<latex> 3.3.5: Proposition 3.3: Determinants of Invertible Matrices </latex>	<latex> $\text{det}A = 0$ if and only if $A$ is not invertible. An equivalent statement $\text{det} A \neq 0$ if and only if $A$ is invertible.  </latex>
<latex> 3.3.2: Theorem 3.4: Determinants of a Transpose </latex>	<latex> For a square matrix, $$\text{det} A = \text{det}(A^{T})$$ </latex>
<latex> 3.3.2: Theorem 3.5: Determinant of a Product </latex>	<latex> For $n \times n$ matrices $A$ and $B$$$\text{det}(AB) = (\text{det}A)(\text{det}B)$$ </latex>
<latex> 3.3.5: Lemma 3.6: Determinant of Matrix and Elementary Matrix Product </latex>	<latex> For a square matrix $A$ and an elementary matrix $E$ of the same size$$\text{det}(AE) = (\text{det}E)(\text{det}E)$$ </latex>
<latex> 3.3.5: Collorary 3.7: Product of Matrix and Any Number of Elementary Matrices </latex>	<latex> <!--anki-->For a square matrix $A$ and any sequence elementary matrices $E_{1}, E_{2}, ... , E_{N}$ of the same size ($n \times n$)$$\text{det}(AE_{1}E_{2}...E_{N}) = (\text{det}A)(\text{det}E_{1})...(\text{det}E_{N})$$ </latex>
<latex> 3.3.5: Lemma 3.8: Origin of Invertible Matrices </latex>	<latex> Any invertible matric is a porduct of elementary matrices.  </latex>
<latex> 3.5.1: Theorem 5.1: Cofactor Expansion of Determinant </latex>	<latex> Let $A$ be an $n \times n$ matrix. For each $j, 1 /leq j /leq n$, determinant of $A$ can be expanded in the row number $j$ as$$\text{det}A = a_{j, 1}(-1)^{j+1}\text{det}A_{j, 1} + a_{j, 2}(-1)^{j+2}\text{det}A_{j, 2} + ... + a_{j, n}(-1)^{j+n}\text{det}A_{j, n} $$Similarly, for each $k, 1 /leq k /leq n$, the determinant can be expanded in thecolumn number $k$,$$\text{det}A = \sum_{j=1}^{n}a_{j, k} (-1)^{j+k} \text{det}A_{j, k}$$ </latex>
<latex> 3.5.1: Definition: Cofactors </latex>	<latex> The numbers$$C_{j, k} = (-1)^{j+k} \text{det}A_{j, k}$$are called cofactors.  </latex>
<latex> 3.5.1: Theorem 5.2: Invertible Matrix in Relations with the Cofactor Matrix </latex>	<latex> Let $A$ be an invertible matrix and let $C$  be its cofactor matrix. Then,$$A^{-1} = \frac{1}{\text{det}A}C^{T}$$ </latex>
<latex> 3.5.1: Corollary 5.3: Cramer's Rule </latex>	<latex> For an invertible matrix $A$ the entry number $k$ of the solution of the equation $A\overrightarrow{x} = \overrightarrow{b}$ is given by the formula$$x_{k} = \frac{\text{det}B_{k}}{\text{det}A}$$where the matrix $B_{k}$ is obtained from $A$ by replacing column number $k$ of $A$ by the vector $\overrightarrow{b}$. </latex>
<latex> 3.6.1: Theorem 6.1: Rank of a Matrix in Terms of Minor </latex>	<latex> For a non-zero matrix $A$ its rank equals to the maximal integer $k$ such that there exists a non-zero minor of order $k$. </latex>
<latex> 3.6.1: Corollary 6.2: Rank of a Polynomial Matrix </latex>	<latex> Let $A = A(x)$ be an $m \times n$ polynomial matrix (i.e. a matrixwhose entries are polynomials of $x$). Then $\text{rank}A(x)$ is constant everywhere, except maybe finitely many points, where the rank is smaller. </latex>
<latex> 4.1.1: Definition: Eigenvalues and Eigenvectors </latex>	<latex> A scalar $\lambda$ is called an eigenvalue of an operator $A: V \to V$ if there exists a non-zero vector $\overrightarrow{v} \in V$ such that$$A\overrightarrow{v} = \lambda \overrightarrow{v}$$The vector $\overrightarrow{v}$ is called the eigenvector of $A$.  </latex>
<latex> 4.1.2: Theorem: Eigenvalue in Relation to Determanant of $A- \lambda I$ </latex>	<latex> $$\lambda \in \sigma (A), \text{i.e. } \lambda \text{ is an eigenvalue of } A \text{ if and only if } \text{det}(A - \lambda I) = 0$$ </latex>
<latex> 4.1.2: Definition: Characteristic Polynomial </latex>	<latex> If $A$ is an $n \times n$ matrix, the determinant $\text{det}(A - \lambda I)$ is a polynomial of degree $n$ of the variable. This polynomial is called the characteristicpolynomial of $A$. </latex>
<latex> 4.1.5: Proposition 1.1: Geometric and Algebraic Multiplicity of an Eigenvalue </latex>	<latex> Geometric multiplicity of an eigenvalue cannot exceed its algebraic multiplicity. </latex>
<latex> 4.1.6: Theorem 1.2: Trace and Determinant in Relation to Eigenvalues </latex>	<latex> Let $A$ be $n \times n$ matrix, and let $ \lambda_{1}, \lambda_{2}, ... \lambda_{n}$ be its complex eigenvalues counting multiplicities. Then$$\text{1. trace}A = \lambda_{1} + \lambda_{2} +  ... + \lambda_{n}$$$$\text{2. det}A = \lambda_{1}\lambda_{2}...\lambda_{n}$$ </latex>
<latex> 4.1.7: Theorem: Eigenvalues of a Triangular Matrix </latex>	<latex> Eigenvalues of a triangular matrix are exactly the diagonal entries.  </latex>
<latex> 4.2.1: Theroem 2.1: Diagonalization  </latex>	<latex> A matrix $A$ (with values in $\mathbb{F}$) admits a representation $A = SDS^{-1}$, where $D$ is a diagonal matrix and $S$ is an invertible one (both withentries in $\mathbb{F}$) if and only if there exists a basis in $\mathbb{F}^{n}$ of eigenvectors of $A$.$\\$Moreover, in this case diagonal entries of $D$ are the eigenvalues and thecolumns of $S$ are the corresponding eigenvectors (column number $k$ corre-sponds to $k$th diagonal entry of $D$). </latex>
<latex> 4.2.1: Remark: Diagonalization Relation to Eigenvalues </latex>	<latex> Note if a matrix admits the representation $A = SDS^{-1}$ with a diagonalmatrix $D$, then a simple direct calculation shows that the columns of$S$ are eigenvectors of $A$ and diagonal entries of $D$ are corresponding eigenvalues. </latex>
<latex> 4.2.3: Theroem 2.2: Linear Independence of Eigenvectors </latex>	<latex> Let $\lambda_{1}, \lambda_{2},..., \lambda_{r}$ be distinct eigenvalues of $A$, and let $\lambda_{1}, \lambda_{2},..., \lambda_{r}$ be the corresponding eigenvectors. Then vectors $\lambda_{1}, \lambda_{2},..., \lambda_{r}$ are linearly independent. </latex>
<latex> 4.2.3: Corollary 2.3: Diagonalizability of Linear Operators with $n = \text{dim}V$ </latex>	<latex> If an operator $A: V \to V$ hjas exactly $n = \text{dim}V$ distinct eigenvalues, then it is diagonalizable.  </latex>
<latex> 4.2.4: Theorem 2.6: Union of the Bases of Linear Independent Subspaces </latex>	<latex> Let $V_{1}, V_{2}, ... , V_{p}$ be a basis of subspaces, and let us have in each subspace $V_{k}$ a basis (of vectors) $\mathcal{B}_{k}$. Then the union $\cup _{k}\mathcal{B}_{k}$ of these bases is a basis in $V$. </latex>
<latex> 4.2.4: Lemma 2.7: Union of the Bases of Linear Independent Subspaces </latex>	<latex> Let $V_{1}, V_{2}, ... , V_{p}$ be a linearly independent family of subspaces, and let us have in each subspace $V_{k}$ a linearly independent system (of vectors) $\mathcal{B}_{k}$. Then the union $B := \cup _{k}\mathcal{B}_{k}$ of these bases is a basis in $V$. </latex>
<latex> 4.2.5: Theorem 2.8: The Geometric and Algebraic Multiplicities of Eigenvalues of Diagonalizable Matricies </latex>	<latex> Let an operator $A : V \to V$ has exactly $n = \text{dim}V$ eigen-values (counting multiplicities). Then $A$ is diagonalizable if and only if for each eigenvalue the dimension of the eigenspace $\text{Ker}(A - \lambda I)$ (i.e the geometic multiplicity of $\lambda$) coincides with the algebraic multiplicity of $\lambda$ </latex>
<latex> 4.2.6: Theorem 2.9: Relation Between Complex Factorization and Eigenvalues and Real Factorization </latex>	<latex> A real $n \times n$ matrix $A$ admits a real factorization (i.e. representation $A = SDS^{-1}$ where $S$ and $D$ are real matrices, $D$ is diagonal and $S$ is invertible) if and only if it admits complex factorization and all eigenvalues of $A$ are real. </latex>
<latex> 5.1.2: Definition: Standard Inner Product of $\mathbb{C}^n$ </latex>	<latex> $$(\overrightarrow{z}, \overrightarrow{w}) = \sum_{k=1}^{n} z_{k}\overline{w_{k}}$$$$\text{Or with notation in hermitian adjoint}$$$$(\overrightarrow{z}, \overrightarrow{w}) = \overrightarrow{w}*\overrightarrow{z}$$ </latex>
<latex> 5.1.3: Innter Product Spaces Properties </latex>	<latex> The inner product is defined as the function that satisifies the following properties:$$\text{1. (Conjugate) symmetry:} (\overrightarrow{x}, \overrightarrow{y}) = \overline{(\overrightarrow{y},\overrightarrow{x})} \text{ note, that for a real space,this property is just symmetry } (\overrightarrow{x}, \overrightarrow{y}) = (\overrightarrow{y},\overrightarrow{x})$$$$\text{ 2. Linearity: } (\alpha\overrightarrow{x} + \beta \overrightarrow{y} , \overrightarrow{z}) = \alpha (\overrightarrow{x}, \overrightarrow{z}) + \beta (\overrightarrow{y}, \overrightarrow{z}) \text{for all vector } \overrightarrow{x}, \overrightarrow{y}, \overrightarrow{z} \text{ and all scalars } \alpha, \beta$$$$\text{3: Non-negativity}(\overrightarrow{x}, \overrightarrow{x})\geq0\text{ for all } \overrightarrow{x}$$$$\text{ 4. Non-degeneracy: } (\overrightarrow{x}, \overrightarrow{x}) = 0 \text{ if and only if } \overrightarrow{x} = 0 $$ </latex>
<latex> 5.1.4: Corollary 1.5: Property of Equilvalent Vectors in Inner Product Spaces </latex>	<latex> Let $\overrightarrow{x}, \overrightarrow{y}$ be vectors in an inner product space $V$. The equality $\overrightarrow{x} = \overrightarrow{y}$ holds if and only if $$(\overrightarrow{x}, \overrightarrow{z}) = (\overrightarrow{y}, \overrightarrow{z}) \text{ for all } \overrightarrow{z} \in V$$ </latex>
<latex> 5.1.4: Corollary 1.6: Operators with Equalivalent Inner Products </latex>	<latex> Suppose two operators $A, B: X \to Y$ satisfy$$(A\overrightarrow{x}, \overrightarrow{y}) = (B\overrightarrow{x}, \overrightarrow{y})$$Then $A = B$. </latex>
<latex> 5.1.4: Theorem 1.7: Cauchy-Schwarz Inequality </latex>	<latex> $$|(\overrightarrow{x}, \overrightarrow{y})| \leq ||\overrightarrow{x}|| ||\overrightarrow{x}||$$ </latex>
<latex> 5.1.4: Lemma 1.8: Triangle Inequality </latex>	<latex> For any vectors $\overrightarrow{x}, \overrightarrow{y}$ in an inner prodct space$$||\overrightarrow{x} + \overrightarrow{y}|| \leq ||\overrightarrow{x}|| + ||\overrightarrow{y}||$$ </latex>
<latex> 5.1.4: Lemma 1.9: Polarization Identities </latex>	<latex> For $\overrightarrow{x}, \overrightarrow{y} \in V$$$(\overrightarrow{x}, \overrightarrow{y}) = \frac{1}{4} (||\overrightarrow{x} + \overrightarrow{y}||^{2} - ||\overrightarrow{x} - \overrightarrow{y}||^{2})$$if $V$ is a real inner product space, and$$(\overrightarrow{x}, \overrightarrow{y}) = \frac{1}{4} \sum_{\alpha = \pm 1, \pm i} \alpha ||\overrightarrow{x} + \alpha \overrightarrow{y}||^{2}$$if $V$ is a complex inner product space.  </latex>
<latex> 5.1.4: Lemma 1.10: Parallelogram Identity </latex>	<latex> For any vectors $\overrightarrow{u}, \overrightarrow{v}$$$||\overrightarrow{u} + \overrightarrow{v}||^{2} + ||\overrightarrow{u} - \overrightarrow{v}||^{2} = 2(||\overrightarrow{u}||^{2} + ||\overrightarrow{v}||^{2})$$ </latex>
<latex> 5.1.5: Definition: Normed Space </latex>	<latex> A vector space $V$ equipped with a norm is called a normal space </latex>
<latex> 5.1.5: Theorem 1.11: Norm Obtained from Inner Product </latex>	<latex> A norm in a normed space is obtained from some inner product if and only if it satisfies the Parallelogram Identity.  </latex>
<latex> 5.2.1: Definition 2.1: Orthogonal </latex>	<latex> Two vectors $\overrightarrow{u}$ and $\overrightarrow{v}$ are called orthogonal (also perpendicular ) if $(\overrightarrow{u}, \overrightarrow{v}) = 0$ </latex>
<latex> 5.2.1: Definition 2.2: Orthogonality to a Subspace </latex>	<latex> We say that a vector $\overrightarrow{v}$ is orthogonal to a subspace $E$ if $\overrightarrow{v}$is orthogonal to all vectors $\overrightarrow{w}$ in $E$. </latex>
<latex> 5.2.1: Lemma 2.3: Perpendicular Vectors to Spanning Vectors </latex>	<latex> Let $E$ be spanned by vectors $\overrightarrow{v_{1}}, \overrightarrow{v_{2}}, ..., \overrightarrow{v_{r}}$ Then $\overrightarrow{v} \perp E$ ifand only if$$\overrightarrow{v} \perp \overrightarrow{v_{k}} \text{ for all } k = 1, 2, ..., r$$ </latex>
<latex> 5.2.1: Definition: Orthogonal System of Vectors  </latex>	<latex> A system of vectors $\overrightarrow{v_{1}}, \overrightarrow{v_{2}}, ..., \overrightarrow{v_{n}}$  is called orthogonal if any two vectors are orthogonal to each other. </latex>
<latex> 5.2.1: Definition: Orthonormal System of Vectors </latex>	<latex> An othogonal system of vectors in which $$||\overrightarrow{v_{k}}|| = 1 \text{ for all } k$$ </latex>
<latex> 5.2.1: Lemma 2.5: Generalize Pythagorean Identity </latex>	<latex> If $\overrightarrow{v_{1}}, ..., \overrightarrow{v_{n}}$ are an orthogonal system of vectors, then$$||\sum_{k=1}^{n}\alpha_{k}\overrightarrow{v_{k}}||^{2} = \sum_{k=1}^{n}|\alpha_{k}|^{2}||\overrightarrow{v_{k}}||^{2}$$ </latex>
<latex> 5.2.1: Corollary 2.6: Linear Dependence of Orthogonal System of Vectors </latex>	<latex> Any orthogonal system of non-zero vectors is linearly independent. </latex>
<latex> 5.2.1: Definition 2.7: Orthogonal and Orthonormal Bases </latex>	<latex> An orthogonal (orthonormal) system whichis also a basis is called an orthogonal (orthonormal) basis. </latex>
<latex> 5.3.1: Definition 3.1: Orthogonal Projection </latex>	<latex> For a vector $\overrightarrow{v}$ its orthogonal projection $P_{E}\overrightarrow{v}$ onto the subspace $E$ is a vector $\overrightarrow{w}$ such that$$\text{1. } \overrightarrow{w} \in E$$$$\text{2. } \overrightarrow{v} - \overrightarrow{w} \perp E$$ </latex>
<latex> 5.3.2: Theorem 3.2: Distance Minimized by Orthogonal Projection </latex>	<latex> The orthogonal projection $\overrightarrow{w} = P_{E}\overrightarrow{v}$ minimizes the distance from $\overrightarrow{v}$ to $E$. For all $\overrightarrow{x} \in E$$$||\overrightarrow{v} - \overrightarrow{w}|| \leq ||\overrightarrow{v} - \overrightarrow{x}||$$But if $\overrightarrow{x} \in E$$$||\overrightarrow{v} - \overrightarrow{w}|| = ||\overrightarrow{v} - \overrightarrow{x}||$$Then $\overrightarrow{x} = \overrightarrow{w}$ </latex>
<latex> 5.3.1: Proposition 3.3:  Formula for Orthogonal Projection </latex>	<latex> Let $\overrightarrow{v_{1}}, ..., \overrightarrow{v_{n}}$ be an orthogonal basis in $E$. Then the orthogonal projection $P_{E}\overrightarrow{v}$ of a vector $\overrightarrow{v}$ is given by the formula$$P_{E}\overrightarrow{v} = \sum_{k=1}^{n} \alpha_{k}\overrightarrow{v_{k}} \text{ where } \alpha_{k} = \frac{(\overrightarrow{v}, \overrightarrow{v_{k}})}{||\overrightarrow{v_{k}}||^{2}}$$In other words$$P_{E}\overrightarrow{v} = frac{(\overrightarrow{v}, \overrightarrow{v_{k}})}{||\overrightarrow{v_{k}}||^{2}}\overrightarrow{v_{k}}$$ </latex>
<latex> 5.3.3: Definition: Orthogonal Complement </latex>	<latex> For a subspace $E$ its orthogonal complement $E^{\perp}$ is the set ofall vectors orthogonal to $E$ </latex>
<latex> 5.3.3: Proposition 3.6: Orthogonal Compliment of the Orthogonal Compliment </latex>	<latex> For a subspace $E$$$(E^{\perp})^{\perp} = E$$ </latex>
<latex> 5.4.2: Theorem 4.1: Null Space Equivalency of Matrix and its Normal Equaton </latex>	<latex> For an $m \times n$ matrix $A$$$\text{Ker}A = \text{Ker}(A^{*}A)$$ </latex>
<latex> 5.4.1: Definition: Normal Equation </latex>	<latex> $$A^{*}A \overrightarrow{x} = A^{*}\overrightarrow{b}$$A solution to this equation gives the least square solution of $A\overrightarrow{x} = \overrightarrow{b}$ </latex>
<latex> 5.4.2: Defintion: Formula for Orthogonal Projection </latex>	<latex> $P_{\text{Ran}A}\overrightarrow{b} = A(A^{*}A)^{-1}A^{*}\overrightarrow{b}$ </latex>
<latex> 5.5.1: Theorem: Identity of Adjoint Matrices in Inner Products </latex>	<latex> $$(A\overrightarrow{x}, \overrightarrow{y}) = (\overrightarrow{x}, A^{*}\overrightarrow{y})$$ </latex>
<latex> 5.5.2: Theorem 5.1: Range and Null Spaces of Adjoint Matrices </latex>	<latex> Let $A: V \to W$ be an operator acting from one inner product space to another. Then$$\text{ 1. } \text{Ker} A^{*} = (\text{Ran} A)^{\perp}$$$$\text{ 2. } \text{Ker} A = (\text{Ran} A^{*})^{\perp}$$$$\text{ 3. } \text{Ran} A= (\text{Ker} A^{*})^{\perp}$$$$\text{ 4. } \text{Ran} A^{*}= (\text{Ker} A)^{\perp}$$ </latex>
<latex> 5.6.1: Definition: Isometry </latex>	<latex> An operator $U: X \to Y$ is called an isometry, if it preservesthe norm$$||U\overrightarrow{x}|| = ||\overrightarrow{x}|| \text{ for all } \overrightarrow{x} \in X$$ </latex>
<latex> 5.6.1: Theorem 6.1: Inner Product of Isometry </latex>	<latex> An operator $U: X \to Y$ is an isometry if and only if it preserves the inner product,$$(\overrightarrow{x}, \overrightarrow{y}) = (U\overrightarrow{x}, U\overrightarrow{y}) \text{ for all } \overrightarrow{x}, \overrightarrow{y} \in X$$ </latex>
<latex> 5.6.1: Lemma 6.2: The Adjoint Times a Isometry </latex>	<latex> An operator $U: X \to Y$ is an isometry if and only if $U^{*}U = I$ </latex>
<latex> 5.6.2: Definition: Unitary Operator </latex>	<latex> An isometry $U: X \to Y$ is called a unitary operator if it is invertible </latex>
<latex> 5.6.1: Proposition 6.3: Dimension of Unitary Operators </latex>	<latex> An isometry $U: X \to Y$ is a unitary operator if and only if $\text{dim}X = \text{dim}Y$ </latex>
<latex> 5.6.3: Proposition 6.4: Properties of Determinant and Eigenvalues of Unitary Matrices </latex>	<latex> Let $U$ be a unitary matrix. Then$$\text{1. }|\text{det}U| = 1 \text{. In particular, for an orthogonal matrix } \text{det}U = \pm 1$$$$\text{2. }\text{If } \lambda \text{ if an eigenvalue of } U \text{, then } |\lambda| = 1$$<br_off /> </latex>
<latex> 5.6.4: Definition: Unitary Equivalent </latex>	<latex> Operators (matrices) $A$ and $B$ are called unitarily equivalent if there exists a unitary operator $U$ such that $A = UBU^{*}$ </latex>
<latex> 5.6.4: Proposition 6.5: Unitary Equivalence to a Diagonal Matrix </latex>	<latex> A matrix $A$ is unitarily equivalent to a diagonal one ifn and only if it has an orthogonal (orthonormal) basis of eigenvectors. </latex>
<latex> Definition: Bijective </latex>	<latex> A map is called bijective if it is both injective and surjective. </latex>
<latex> Definition: Injective </latex>	<latex> If a function $f$ is defined on value in a set $A$ and takes values in a set $B$, $f$ is said to be injective if whenever $f(x) = f(y)$, $x =y$. In other words, it maps distinct object to distinct objects or is said to be one to one.  </latex>
<latex> Kernal of Injective Linear Transformation </latex>	<latex> A linear transformation is injective if and only if its kernal is only the zero vector.  </latex>
<latex> Definition: Surjective </latex>	<latex> A function $f$ that is defined on values in the set $A$ and takes values in the set $B$ is said to be surjective if for any $\overrightarrow{b} \in B$ there exists an $\overrightarrow{a} \in A$ such that $f(\overrightarrow{a}) = \overrightarrow{b}$. $f$ is then also said to be onto.  </latex>
